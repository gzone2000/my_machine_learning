{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_2.4_Exercise_QL_Taxi.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F5tABggTVKQw"},"source":["# SARSA & Q - Learning\n","\n","# 연습데이터 : Taxi\n","![좋은거](https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png)\n","\n","**학습목표**\n","\n","1. SARSA의 업데이트 맛 본다.\n","2. Q-learning의 업데이트를 맛 본다.\n","3. 일단 코드 짤 수 있다."]},{"cell_type":"markdown","metadata":{"id":"uJB96tIRZfeT"},"source":["# 필요 라이브러리 불러오기\n","\n","1. 이런 연습에서는 딱히 비디오 영상이 필요하지 않다."]},{"cell_type":"code","metadata":{"id":"Bi_Z3_kaZRot","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613967296775,"user_tz":-540,"elapsed":3608,"user":{"displayName":"yunwoo oh","photoUrl":"","userId":"01035531558844683999"}},"outputId":"b0aeb801-ef4e-404a-f536-9fd1397379f1"},"source":["!pip install gym"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MVEK7ArIZoeN","executionInfo":{"status":"ok","timestamp":1613967296778,"user_tz":-540,"elapsed":1022,"user":{"displayName":"yunwoo oh","photoUrl":"","userId":"01035531558844683999"}}},"source":["import numpy as np\n","import gym"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttRBaI2WZqcX"},"source":["# 사용할 환경 불러오기\n","[택시!](https://gym.openai.com/envs/Taxi-v3/)\n"]},{"cell_type":"code","metadata":{"id":"86nb4VeAaACM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613967300684,"user_tz":-540,"elapsed":1081,"user":{"displayName":"yunwoo oh","photoUrl":"","userId":"01035531558844683999"}},"outputId":"a30e473a-57d1-427b-a243-adde249d66ed"},"source":["env = gym.make('Taxi-v3')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["observation space: Discrete(500)\n","action space: Discrete(6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vrgrAKT09lsD"},"source":["# SARSA와, Q-learning 준비 해보기\n","\n","**핵심 코드**\n","1. SARSA : S,A,R,S,A 가 끝나면 Q업데이트!\n","```\n","Q[s,a] = Q[s,a] + alpha * ((r + gama* Q[s1,a1]) - Q[s,a])\n","```\n","2. Q-Learning : S,A,R,S 후 max A, 그다음 Q업데이트!\n","```\n","Q[s,a] = Q[s,a] + alpha*(r + gama*np.max(Q[s1,:]) - Q[s,a])\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"epY6RCn6vxTi"},"source":["# II. Q-Learning 연습\n"]},{"cell_type":"code","metadata":{"id":"AQ3qf8YSwFz-","executionInfo":{"status":"ok","timestamp":1613961707326,"user_tz":-540,"elapsed":614,"user":{"displayName":"yunwoo oh","photoUrl":"","userId":"01035531558844683999"}}},"source":["alpha = 0.2\n","gamma = 0.999 # 할인율!\n","n_episod = 20000\n","epsilon = 0.15"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"8aoMzJ0IwF0B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613967505651,"user_tz":-540,"elapsed":19846,"user":{"displayName":"yunwoo oh","photoUrl":"","userId":"01035531558844683999"}},"outputId":"f801e931-4441-4729-80f4-3c72b29bcf86"},"source":["Q = np.zeros([env.observation_space.n,\n","              env.action_space.n])\n","print(Q.shape)\n","\n","rewards_ql = []\n","\n","for i in range(n_episod) : \n","    s0 = env.reset()\n","    done = False\n","\n","    while True :\n","        # e-greedy 하게 행동을 선택하여 a0에 선언하자.\n","        if np.random.uniform() < epsilon :\n","          a0 = env.action_space.sample()\n","        else :\n","          greedy_actions = np.argwhere(Q[s0, :] == np.amax(Q[s0, :])).reshape(-1)\n","          a0 = np.random.choice(greedy_actions)\n","\n","        # a0 를 이용하여 행동을 하고\n","        # s1, r1, done, _ 를 새로 선언하다.  step!\n","        s1, r1, done, _ = env.step(a0)\n","\n","        # ( s0, a0, r1, s1 ) 로 부터! 차이점 주의!\n","        # 업데이트 코드를 작성하자.\n","        Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*np.max(Q[s1,:]) - Q[s0, a0])\n","\n","        if done == True: # 종료되었으면\n","            rewards_ql.append(r1)\n","            env.close() # 환경 닫고.\n","            break # 멈춰야지\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","\n","    if (i+1) % 4000 == 0 :\n","        print('===========  에피소드 : {}  ============'.format(i+1))\n","        # for rows in Q.reshape([-1,5,5]):\n","        #     print(*rows)\n","        "],"execution_count":8,"outputs":[{"output_type":"stream","text":["(500, 6)\n","===========  에피소드 : 4000  ============\n","===========  에피소드 : 8000  ============\n","===========  에피소드 : 12000  ============\n","===========  에피소드 : 16000  ============\n","===========  에피소드 : 20000  ============\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ImORkklkwF0D"},"source":["### 학습 완료된 것 구경\n","\n","위 에서, 학습을 1 ~ 2회만 시켜보고 아래를 해봐도 좋다."]},{"cell_type":"code","metadata":{"id":"4-BKSI37wF0E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613967576995,"user_tz":-540,"elapsed":589,"user":{"displayName":"yunwoo oh","photoUrl":"","userId":"01035531558844683999"}},"outputId":"727315b9-c582-4ffc-82e1-3defb02c63c0"},"source":["s0 = env.reset()\n","done = False\n","a0 = env.action_space.sample() # 처음엔 랜덤무브\n","\n","while True :\n","    env.render()\n","    s1, r1, done, _ = env.step(a0) # 일단 움직인다.\n","\n","    greedy_actions = np.argwhere(Q[s1, :] == np.amax(Q[s1, :])).reshape(-1)\n","    a1 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","    if done == True: # 종료되었으면\n","        env.render()\n","        env.close() # 환경 닫고.\n","        break # 멈춰야지\n","\n","    s0 = s1 # 다음 루프에선 이것이 직전 state\n","    a0 = a1 # 다음 루프에선 이것이 직전 action"],"execution_count":9,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[34;1mR\u001b[0m: | : :G|\n","| :\u001b[43m \u001b[0m| : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","\n","+---------+\n","|\u001b[34;1mR\u001b[0m: | : :G|\n","| :\u001b[43m \u001b[0m| : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (Dropoff)\n","+---------+\n","|\u001b[34;1mR\u001b[0m: | : :G|\n","|\u001b[43m \u001b[0m: | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (West)\n","+---------+\n","|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (North)\n","+---------+\n","|\u001b[42mR\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (Pickup)\n","+---------+\n","|R: | : :G|\n","|\u001b[42m_\u001b[0m: | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (South)\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","|\u001b[42m_\u001b[0m: : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (South)\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","| :\u001b[42m_\u001b[0m: : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (East)\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","| : :\u001b[42m_\u001b[0m: : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (East)\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : :\u001b[42m_\u001b[0m: |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (East)\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : |\u001b[42m_\u001b[0m: |\n","|Y| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","  (South)\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n","+---------+\n","  (South)\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n","+---------+\n","  (Dropoff)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QTjjfmS6wGoC"},"source":["### Q-learning 학습 코드를 보고 진행 순서를 정리하자."]},{"cell_type":"markdown","metadata":{"id":"18kFKiF-9xZm"},"source":["1. env.reset()을 통해 state 확인\r\n","2. epsilon 15% 확률로 random action(exploration)이나 최적 action(exploitation) 선택\r\n","  * 최적 action을 경우 \r\n","  * (sarsa)\r\n","    * 매 action 마다 ( s0, a0, r1, s1, a1 ) 로 부터 아래와 같이 Q 테이블 업데이트\r\n","    * Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*Q[s1, a1] - Q[s0, a0])\r\n","  * (Q-learning)\r\n","    * 매 action 마다 ( s0, a0, r1, s1 ) 로 부터 아래와 같이 Q 테이블 업데이트\r\n","    * Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*np.max(Q[s1,:]) - Q[s0, a0])\r\n","3. env.step을 통해 다음 state, 다음 reward를 확인\r\n","4. done == True 이면 종료"]}]}