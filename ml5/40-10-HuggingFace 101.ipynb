{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"04-HuggingFace 101.ipynb","provenance":[],"collapsed_sections":["wCyhd7qDSnqr"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"N7NhvLJNU6iq"},"source":["# HuggingFace\n","\n","- https://techcrunch.com/2021/03/11/hugging-face-raises-40-million-for-its-natural-language-processing-library/"]},{"cell_type":"markdown","metadata":{"id":"hSgq2Vi8SnqX"},"source":["# Install & Import"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ciWPtL9lSnqZ"},"source":["!pip install transformers\n","!pip install --upgrade keras\n","!pip install --upgrade tensorflow\n","!pip install datasets\n","!pip install bertviz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJytliQzVWNc"},"source":["- 비슷한 형태로 한국어 dataset을 제공하는 python library\n","  - `pip install Korpora`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NfiUi6fzSnqa"},"source":["# Introduction to HuggingFace\n","\n","https://huggingface.co/transformers/index.html\n","\n","**Contents**\n","\n","1. Model & Tasks\n","2. Loading Pre-Trained Models\n","3. Fine-Tuning Models\n","4. Interpreting Your Model"]},{"cell_type":"markdown","metadata":{"id":"lt90bdy4Snqb"},"source":["# 1. Models & Tasks\n"," "]},{"cell_type":"markdown","metadata":{"id":"c-s2D8boSnqc"},"source":["### Models\n","(https://huggingface.co/transformers/model_summary.html)\n","\n","   - **Autoregressive models:** Autoregressive models are pretrained on the classic language modeling task: guess the next token having read all the previous ones. They correspond to the *decoder* of the original transformer model, and a mask is used on top of the full sentence so that the attention heads can only see what was before in the text, and not what’s after. Although those models can be fine-tuned and achieve great results on many tasks, the most natural application is text generation. A typical example of such models is GPT.\n","   - **Autoencoding models:** Autoencoding models are pretrained by corrupting the input tokens in some way and trying to reconstruct the original sentence. They correspond to the encoder of the original transformer model in the sense that they get access to the full inputs without any mask. Those models usually build a bidirectional representation of the whole sentence. They can be fine-tuned and achieve great results on many tasks such as text generation, but their most natural application is sentence classification or token classification. A typical example of such models is BERT.\n","   - **Sequence-to-sequence models:** Sequence-to-sequence models use both the encoder and the decoder of the original transformer, either for translation tasks or by transforming other tasks to sequence-to-sequence problems. They can be fine-tuned to many tasks but their most natural applications are translation, summarization and question answering. The original transformer model is an example of such a model (only for translation), T5 is an example that can be fine-tuned on other tasks.\n","   - **Multimodal models:** Multimodal models mix text inputs with other kinds (e.g. images) and are more specific to a given task.\n","   - **Retrieval-based models:** Some models use documents retrieval during (pre)training and inference for open-domain question answering, for example\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HcXoAN0bV7KH"},"source":["### Tasks\n","(https://huggingface.co/transformers/task_summary.html)\n","\n","   - **Sequence Classification:** classifying sequences according to a given number of classes. (ex. GLUE)\n","   - **Extractive Question Answering:** extracting an answer from a text given a question (ex. SQUAD)\n","   - **Language Modeling:** task of fitting a model to a corpus, which can be domain specific\n","       - **Masked Language Modeling:** task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token. (ex. BERT pre-training)\n","       - **Causal Language Modeling:** predicting the token following a sequence of tokens (ex. GPT-2)\n","   - **Text Generation:** create a coherent portion of text that is a continuation from the given context\n","   - **Named Entity Recognition (Token Classification):** classifying tokens according to a class, for example, identifying a token as a person, an organisation or a location\n","   - **Summarization:** task of summarizing a document or an article into a shorter text\n","   - **Translation:** task of translating a text from one language to another\n","   \n"]},{"cell_type":"markdown","metadata":{"id":"sJr7vXNHSnqc"},"source":["## Pipeline\n","(https://huggingface.co/transformers/main_classes/pipelines.html)\n","\n","The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including **Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.** See the task summary for examples of use. \n","\n","- ConversationalPipeline, FeatureExtractionPipeline, FillMaskPipeline, QuestionAnsweringPipeline, SummarizationPipeline, TextClassificationPipeline, TextGenerationPipeline, TokenClassificationPipeline, TranslationPipeline, ZeroShotClassificationPipeline, Text2TextGenerationPipeline, TableQuestionAnsweringPipeline"]},{"cell_type":"code","metadata":{"id":"L5oBWaI6Snqd"},"source":["from transformers import pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awZ-x2BRSnqd"},"source":["### Sequence Classification"]},{"cell_type":"code","metadata":{"id":"OWYZ5buJSnqd"},"source":["# Sequence Classification\n","classifier = pipeline('sentiment-analysis')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wXjvtsQSnqd"},"source":["print(classifier(\"I love you\")[0])\n","print(classifier(\"I hate you\")[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g-f9YhYySnqe"},"source":["### Question Answering"]},{"cell_type":"code","metadata":{"id":"Hx9IKXmlSnqe"},"source":["# Question Answering\n","qa = pipeline(\"question-answering\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wcHvDvSWSnqf"},"source":["context = r\"\"\"\n","Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n","question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n","a model on a SQuAD task, you may leverage the `run_squad.py`.\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGdIkctWSnqf"},"source":["print(qa(question=\"What is extractive question answering?\", context=context))\n","print(qa(question=\"What is a good example of a question answering dataset?\", context=context))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EOo8EGsBSnqf"},"source":["### Text Generation"]},{"cell_type":"code","metadata":{"id":"RrwUZH8aSnqg"},"source":["text_generator = pipeline(\"text-generation\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9_OxpU9ySnqg"},"source":["print(text_generator(\"When the Titanic crashed, I\", max_length=50, do_sample=False))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvrJj1YlSnqh"},"source":["Try Language Modeling, Token Classification and more on your own..."]},{"cell_type":"markdown","metadata":{"id":"6JGk9zicSnqh"},"source":["# 2. Loading Pre-Trained Models\n","\n","We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions, just three standard classes required to use each model: \n","1. configuration, \n","2. models and \n","3. tokenizer.\n","\n","- [Models](https://huggingface.co/models)"]},{"cell_type":"code","metadata":{"id":"guI0a9UySnqh"},"source":["from transformers import AutoConfig, AutoTokenizer, AutoModel, AutoModelForSequenceClassification"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vn-0kuhBSnqi"},"source":["### Configuration\n","\n","The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace’s AWS S3 repository).\n","\n","`classmethod: .from_pretrained(pretrained_model_name_or_path, **kwargs)`"]},{"cell_type":"code","metadata":{"id":"5EGuxnP7Snqi"},"source":["# Set Keyword Args \n","config_args = {'hidden_dropout_prob':0.2, \n","              'num_labels':2}\n","\n","# Or.. load from path\n","# config_path = './model/checkpoint.ckpt.index'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3D0wmc0Snqi"},"source":["model_name = 'xlm-roberta-base'\n","config = AutoConfig.from_pretrained(model_name, **config_args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"UyZUot-1Snqk"},"source":["config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5mAb3cJmSnqk"},"source":["### Models\n","\n","- The base classes PreTrainedModel, TFPreTrainedModel, and FlaxPreTrainedModel implement the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace’s AWS S3 repository).\n","\n","- Automodel is a generic model class that will be instantiated as one of the base model classes of the library. For specific tasks, load AutoModelForTASK."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"2GIIvYS6Snql"},"source":["model_config = AutoModel.from_config(config)\n","model_pretrained = AutoModel.from_pretrained(model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1G6fvekrSnql"},"source":["# Notice Dropout(p=0.2)\n","model_config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkp1ebCrSnqm"},"source":["# Notice Dropout(p=0.1)\n","model_pretrained"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"OIrUgpsfSnqn"},"source":["config = AutoConfig.from_pretrained(model_name, **config_args)\n","model_seq = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-dUG-ieSnqn"},"source":["# Instead of Pooling Layer, Classification Layer added on model\n","model_seq"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9jsj1RxDSnqn"},"source":["### Tokenizer\n","\n","A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a “Fast” implementation based on the Rust library tokenizers. The “Fast” implementations allows:\n","\n","1. a significant speed-up in particular when doing batched tokenization and\n","\n","2. additional methods to map between the original string (character and words) and the token space (e.g. getting the index of the token comprising a given character or the span of characters corresponding to a given token). Currently no “Fast” implementation is available for the SentencePiece-based tokenizers (for T5, ALBERT, CamemBERT, XLMRoBERTa and XLNet models).\n","\n","`classmethod: from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)`"]},{"cell_type":"code","metadata":{"id":"U7dEeIolSnqo"},"source":["tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2PK6LBYSnqo"},"source":["tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"t-ztpSRKSnqo"},"source":["print(tokenizer.tokenize('그 영화는 재밌었다'))\n","print(tokenizer('그 영화는 재밌었다'))\n","print(tokenizer(['그 영화는 재밌었다','나는 별로였다']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D81_zzQkSnqp"},"source":["### Using Fine-Tuned models in Pipeline\n","\n","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"]},{"cell_type":"markdown","metadata":{"id":"38Zv8GRjSnqp"},"source":["#### Sentiment Classification (EN)"]},{"cell_type":"code","metadata":{"id":"PTBo3cfqSnqp"},"source":["# Use task specific fine-tuned model in Pipeline\n","# fine-tuned model that predicts the sentiment of the review as a number of stars (between 1 and 5).\n","model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" \n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","pipe = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ve73e9-ySnqq"},"source":["print(pipe(\"Je t'adore\")) # I love you\n","print(pipe(\"Je te deteste\")) # I hate you"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QCrL0VmSnqq"},"source":["# 3. Fine-Tuning Models\n","\n","https://huggingface.co/transformers/examples.html"]},{"cell_type":"markdown","metadata":{"id":"yhg7tya_Snqq"},"source":["## Fine Tuning Example: NSMC\n","\n","Naver Sentiment Movie Classification\n","https://huggingface.co/datasets/nsmc"]},{"cell_type":"code","metadata":{"id":"uMSpiPS7Snqr"},"source":["from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","\n","model_path_or_name = 'xlm-roberta-base'\n","config = AutoConfig.from_pretrained(model_path_or_name, num_labels=2)\n","model = AutoModelForSequenceClassification.from_pretrained(model_path_or_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_path_or_name, config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wCyhd7qDSnqr"},"source":["### Load Custom Dataset\n","\n","The datasets.Dataset object behaves like a normal python container. You can query its length, get rows, columns and also lot of metadata on the dataset (description, citation, split sizes, etc).\n","\n","https://huggingface.co/docs/datasets/exploring.html"]},{"cell_type":"markdown","metadata":{"id":"w-b224x7Snqr"},"source":["### 1. Using Datasets \n","\n","https://huggingface.co/datasets/nsmc"]},{"cell_type":"code","metadata":{"id":"RvLEPWbZSnqr"},"source":["from datasets import load_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"PudW_lUNSnqs"},"source":["dataset = load_dataset('nsmc')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"shIVTefFSnqs"},"source":["dataset['train'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8NBpe0dSnqs"},"source":["dataset['test'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3DSe9M9Snqt"},"source":["import torch\n","class nsmc_data(torch.utils.data.Dataset):\n","    def __init__(self, dataset, tokenizer):\n","        super().__init__()\n","        self.label = torch.tensor(dataset['label']).long()\n","        features = tokenizer([str(x) for x in dataset['document']], padding=True, truncation=True)\n","        self.input = torch.tensor(features['input_ids'])\n","        self.mask = torch.tensor(features['attention_mask'])\n","    \n","    def __len__(self):\n","        return len(self.input)\n","\n","    def __getitem__(self, index):\n","        return {'input_ids': self.input[index], 'attention_mask': self.mask[index], 'label':self.label[index]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9j_FivorSnqt"},"source":["train_data = nsmc_data(dataset['train'], tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCBHdsp0Snqt"},"source":["test_data = nsmc_data(dataset['test'], tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_vehbISSnqv"},"source":["### Load Trainer \n","\n","The **Trainer** and **TFTrainer** classes provide an API for feature-complete training in most standard use cases. It’s used in most of the example scripts.\n","\n","Before instantiating your Trainer/TFTrainer, create a TrainingArguments/TFTrainingArguments to access all the points of customization during training."]},{"cell_type":"code","metadata":{"id":"2DMScyIvSnqv"},"source":["from transformers import TrainingArguments, Trainer\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IO70B3TtSnqv"},"source":["def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LDDBTD-USnqw"},"source":["training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=1, \n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=64,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"8vcaMLrhSnqw"},"source":["training_args"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eM-22kUSnqw"},"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_data,\n","    eval_dataset=test_data\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"sZpnyz9ESnqw"},"source":["trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTnd2m-vSnqx"},"source":["trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ben1Tm_USnqx"},"source":["#model.save_pretrained('./nsmc_model')\n","tokenizer.save_pretrained('./nsmc_tokenzier')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oUA3QQGwSnqy"},"source":["# 4. Interpreting Your Model"]},{"cell_type":"markdown","metadata":{"id":"q4v0F5CDSnqy"},"source":["### Looking at Attention"]},{"cell_type":"code","metadata":{"id":"VMQhU6a3Snqy"},"source":["from bertviz import head_view"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7MnESNBLSnqz"},"source":["model_path_or_name = './nsmc_model'\n","model = AutoModelForSequenceClassification.from_pretrained(model_path_or_name, output_attentions=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dA2ZkPvFSnqz"},"source":["sentence = \"이 영화는 아주 재미있다.\"\n","inputs = tokenizer.encode_plus(sentence, return_tensors='pt', add_special_tokens=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"dzpwJdMYSnqz"},"source":["input_ids = inputs['input_ids'].to(model.device)\n","attention = model(input_ids)[-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eawgrizrSnqz"},"source":["input_id_list = input_ids[0].tolist() # Batch index 0\n","tokens = tokenizer.convert_ids_to_tokens(input_id_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOFHoVAaSnq0"},"source":["input_id_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"63Im7uS5Snq0"},"source":["tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6mkT_nRSSnq0"},"source":["head_view(attention, tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FmMcny7OSnq1"},"source":["### Error Analysis"]},{"cell_type":"code","metadata":{"id":"7halLNTiSnq1"},"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adJQJEkhSnq2"},"source":["#Get Predictions as Array\n","preds = trainer.predict(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGVRvDTOSnq2"},"source":["#Argmax Softmax Values\n","predictions = np.argmax(preds[0], axis=1)\n","labels = preds[1]\n","print(predictions)\n","print(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pUzDNyfSnq2"},"source":["#Confusion Matrix\n","\n","cm = confusion_matrix(labels,predictions)\n","df_cm = pd.DataFrame(cm, index = [\"Positive\", \"Negative\"],\n","                  columns = [\"Positive\", \"Negative\"])\n","sn.heatmap(df_cm, annot=True, fmt=\".0f\").set(title=\"Confusion Matrix\", xlabel=\"Predicted\", ylabel=\"Observed\",)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g42Us0WjSnq3"},"source":["(labels == 1)&(predictions == 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I07hjJC3Snq3"},"source":["test_dataset = pd.read_csv(test_file_path, sep='\\t', quoting=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"-W32Dd0dSnq3"},"source":["# False Negative\n","test_dataset['document'][(labels == 1)&(predictions == 0)].sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TV984U2ASnq4"},"source":["# False Positive\n","test_dataset['document'][(labels == 0)&(predictions == 1)].sample(10)"],"execution_count":null,"outputs":[]}]}