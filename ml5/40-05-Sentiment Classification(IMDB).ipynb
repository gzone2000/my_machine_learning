{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05-Sentiment Classification.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"112iqqjyfGjrjI8_WE04uD2buakxsMeKF","authorship_tag":"ABX9TyMaNQrFxFhUhmIO1BcHyJ5I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"I4H72nPYqAwV"},"source":["import os, random\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torchtext import data, datasets\n","from torchtext.legacy.data import BucketIterator\n","from torchtext.legacy import data\n","from torchtext.legacy.data import TabularDataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0Yg9b8XqJtl"},"source":["path = \"/content/drive/MyDrive/3.AI_트랜스포머_이재원 강사님(3.24~3.26)/data\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZoEN1JIrE5d"},"source":["## 1. Load Dataset and Split"]},{"cell_type":"code","metadata":{"id":"RMze1J1kqZZ8"},"source":["df = pd.read_csv(os.path.join(path, 'IMDb_Reviews.csv'))\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yk47P55KqdDX"},"source":["print(len(df))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQnHl6OMqhKK"},"source":["train_df = df[:int(len(df)*0.8)]\n","test_df = df[int(len(df)*0.8):]\n","\n","train_df.to_csv(os.path.join(path, \"IMDb_train_data.csv\"), index=False)\n","test_df.to_csv(os.path.join(path, \"IMDb_test_data.csv\"), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"880tm-Mzq6Wz"},"source":["## 2. Define TorchText Field "]},{"cell_type":"markdown","metadata":{"id":"T48wpK7pszkC"},"source":["- sequential : 시퀀스 데이터 여부. (True가 기본값)\n","- use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)\n","- tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)\n","- lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)\n","batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부. (False가 기본값)\n","- is_target : 레이블 데이터 여부. (False가 기본값)\n","- fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다.\n","- include_lengths : 토크나이즈된 문장의 길이를 같이 return 한다(True 또는 False)"]},{"cell_type":"code","metadata":{"id":"wQi4GInqq03e"},"source":["# 필드 정의\n","TEXT = data.Field(sequential=True,\n","                  use_vocab=True,\n","                  tokenize=lambda x: x.split(),\n","                  lower=True,\n","                  batch_first=True,\n","                  include_lengths=True)\n","\n","LABEL = data.LabelField(dtype = torch.float)\n","\n","# LABEL = data.Field(sequential=False,\n","#                    use_vocab=False,\n","#                    batch_first=False,\n","#                    is_target=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UID43-Tvs4Wn"},"source":["## 3. Create Dataset"]},{"cell_type":"code","metadata":{"id":"UKR3b166s7f6"},"source":["train_data, test_data = TabularDataset.splits(\n","    path=path,\n","    train='IMDb_train_data.csv', test='IMDb_test_data.csv', format='csv',\n","    fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(1234))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XIxmrHdMtCno"},"source":["print(vars(train_data[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTNAy--atkYU"},"source":["print(train_data.fields.items())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qXLS9KfLt4eG"},"source":["## 4. Create Vocabulary"]},{"cell_type":"markdown","metadata":{"id":"3FBK5KiNv5jr"},"source":["- torchtext에서 제공하는 pretrained language model이 있다. 그 중 glove를 사용한다. \n","- unk_init은 unk 토큰을 어떻게 initialize할지를 나타낸다. "]},{"cell_type":"code","metadata":{"id":"pcRrZMAItq6-"},"source":["TEXT.build_vocab(train_data,\n","                min_freq=10, \n","                max_size=1000)\n","                 #  vectors = \"glove.6B.100d\",)\n","\n","LABEL.build_vocab(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-D5vce7t1rc"},"source":["print(len(TEXT.vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IkkZqk-uAFW"},"source":["print(TEXT.vocab.stoi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eGFw6_4SuL25"},"source":["## 5. DataLoader for TorchText"]},{"cell_type":"code","metadata":{"id":"NOsvwfIbuPbg"},"source":["BATCH_SIZE = 16\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    sort_key = lambda x: len(x.text),\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2nA8LmIuUpf"},"source":["batch = next(iter(train_iterator))\n","print(type(batch))\n","print(batch)\n","print(batch.text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6cAAIcV8wUD6"},"source":["## 6. Model Architecture"]},{"cell_type":"code","metadata":{"id":"VOd2zxXHwTbn"},"source":["class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text, text_lengths):\n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        # pack sequence\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), batch_first=True)        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","        \n","        #unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3lITaN4ytHh"},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8D1hnDbyw6c"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9P8BIvqPyywy"},"source":["# pretrained_embeddings = TEXT.vocab.vectors\n","\n","# print(pretrained_embeddings.shape)\n","# model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWRSj2Qby0L-"},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t454yuOgy7Pz"},"source":["## 7. Train & Evaluate Model"]},{"cell_type":"code","metadata":{"id":"4I0Bzq7uy5gn"},"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.BCEWithLogitsLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sRgkw061zBpV"},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTuFdomgzDsw"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        text, text_lengths = batch.text\n","        predictions = model(text, text_lengths).squeeze(1)\n","        loss = criterion(predictions, batch.label)\n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hloKVOuHzFay"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            text, text_lengths = batch.text\n","            \n","            predictions = model(text, text_lengths).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUnjs17zzHRE"},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWZpWs8MzIxO"},"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), os.path.join(path, 'rnn-imdb-sentiment.pt'))\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4k3top_azKo0"},"source":["model.load_state_dict(torch.load(os.path.join(path, 'rnn-imdb-sentiment.pt')))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbnXghEs4_zQ"},"source":[""],"execution_count":null,"outputs":[]}]}