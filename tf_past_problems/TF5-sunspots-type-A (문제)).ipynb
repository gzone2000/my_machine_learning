{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF5-sunspots-type-A (문제)","provenance":[{"file_id":"1e6n13XxL6itwRWJ6EDzEbNlWJVK_7DAx","timestamp":1599033591112}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ovj4aJhgiNLR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599144833090,"user_tz":-540,"elapsed":425099,"user":{"displayName":"yunwoo oh","photoUrl":"","userId":"01035531558844683999"}},"outputId":"e72489dc-2b7b-4843-c54c-ee05a252c682"},"source":["# ======================================================================\n","# There are 5 questions in this test with increasing difficulty from 1-5\n","# Please note that the weight of the grade for the question is relative\n","# to its difficulty. So your Category 1 question will score much less\n","# than your Category 5 question.\n","# ======================================================================\n","#\n","# Sequence Modelling Question.\n","#\n","# For this task you will need to train a neural network\n","# to predict sunspot activity using the Sunspots.csv\n","# provided. Your neural network is expected to have an MAE\n","# of at least 20, with top marks going to one with an MAE\n","# of around 15. At the bottom is provided some testing\n","# code should you want to check before uploading which measures\n","# the MAE for you. Strongly recommend you test your model with\n","# this to be able to see how it performs.\n","\n","\n","# =================================================== #\n","# =================================================== #\n","# ====== 모델링 할 때는 포함, 시험 볼때는 제거 ====== #\n","# 텐서플로우 버전 충돌로 인하여 \n","# 반드시 tensorflow 버전을 2.1.0 버전 설치 후 \n","# 모델작업 진행 해주셔야 합니다.\n","# 시험보실 때는 아래 \n","# !pip install tensorflow==2.1.0 명령어 필요 없습니다\n","# 텐서플로우 2.1.0 버전 재설치 코드\n","!pip install tensorflow==2.1.0\n","\n","# ====== 모델링 할 때는 포함, 시험 볼때는 제거 ====== #\n","# =================================================== #\n","\n","# =========== 합격 기준 가이드라인 공유 ============= #\n","# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n","# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n","# =================================================== #\n","# 문제명: Category 5 - sunspots type A (with Lambda)\n","# val_loss: 상관없음\n","# val_mae: 13.50\n","# =================================================== #\n","# =================================================== #\n","\n","\n","import csv\n","import tensorflow as tf\n","import numpy as np\n","import urllib\n","from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D, Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.losses import Huber\n","\n","\n","# DO NOT CHANGE THIS CODE\n","def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n","    series = tf.expand_dims(series, axis=-1)\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n","    ds = ds.shuffle(shuffle_buffer)\n","    ds = ds.map(lambda w: (w[:-1], w[1:]))\n","    return ds.batch(batch_size).prefetch(1)\n","\n","\n","def solution_model():\n","    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n","    urllib.request.urlretrieve(url, 'sunspots.csv')\n","\n","\t# Your data should be loaded into 2 Python lists called time_step\n","\t# and sunspots. They are decleared here.\n","    time_step = []\n","    sunspots = []\n","\n","    with open('sunspots.csv') as csvfile:\n","      reader = csv.reader(csvfile, delimiter=',')\n","      next(reader)\n","      for row in reader:\n","        sunspots.append(float(row[2]))\n","        time_step.append(int(row[0]))\n","\n","\n","\t# You should use numpy to create \n","\t# - your series from the list of sunspots\n","\t# - your time details from the list of time steps\n","    series = np.array(sunspots)\n","    time = np.array(time_step)\n","\n","\t# You should split the dataset into training and validation splits\n","\t# At time 3000. So everything up to 3000 is training, and everything\n","\t# after 3000 is validation. Write the code below to achieve that.\n","    split_time = 3000\n","    time_train = time[:split_time]\n","    time_valid = time[split_time:]\n","    x_train = series[:split_time]\n","    x_valid = series[split_time:]\n","\n","\n","    # DO NOT CHANGE THIS CODE\n","    window_size = 30\n","    batch_size = 32\n","    shuffle_buffer_size = 1000\n","\n","\n","    tf.keras.backend.clear_session()\n","    # You can use any random seed you want. We use 51. :)\n","    tf.random.set_seed(51)\n","    np.random.seed(51)\n","    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n","    validation_set = windowed_dataset(x_valid, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n","\n","\n","    model = tf.keras.models.Sequential([\n","      tf.keras.layers.Conv1D(60, kernel_size=5,\n","                          padding=\"causal\",\n","                          activation=\"relu\",\n","                          input_shape=[None, 1]),\n","      tf.keras.layers.LSTM(60, return_sequences=True),\n","      tf.keras.layers.LSTM(60, return_sequences=True),\n","      Dropout(0.4),\n","      tf.keras.layers.Dense(32, activation=\"relu\"),\n","      tf.keras.layers.Dense(16, activation=\"relu\"),\n","\n","      # YOUR CODE HERE. DO NOT CHANGE THE FINAL TWO LAYERS FROM BELOW\n","      tf.keras.layers.Dense(1),\n","      # The data is not normalized, so this lambda layer helps\n","      # keep the MAE in line with expectations. Do not modify.\n","      tf.keras.layers.Lambda(lambda x: x * 400)\n","    ])\n","\n","    model.summary()\n","\n","    optimizer = SGD(lr=1e-5, momentum=0.9)\n","    loss= Huber()\n","    model.compile(loss=loss,\n","              optimizer=optimizer,\n","              metrics=[\"mae\"])\n","\n","    checkpoint_path = 'tmp_checkpoint.ckpt'\n","    checkpoint = ModelCheckpoint(checkpoint_path, \n","                                save_weights_only=True, \n","                                save_best_only=True, \n","                                monitor='val_mae',\n","                                verbose=1)\n","\n","    epochs=100\n","    history = model.fit(train_set, \n","                    validation_data=(validation_set), \n","                    epochs=epochs, \n","                    callbacks=[checkpoint],\n","                   )\n","    model.load_weights(checkpoint_path)\n","\n","    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n","    return model\n","\n","\n","# Note that you'll need to save your model as a .h5 like this\n","# This .h5 will be uploaded to the testing infrastructure\n","# and a score will be returned to you\n","if __name__ == '__main__':\n","    model = solution_model()\n","    model.save(\"TF5-sunspots-type-A.h5\")\n","\n","\n","\n","# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n","# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n","#def model_forecast(model, series, window_size):\n","#    ds = tf.data.Dataset.from_tensor_slices(series)\n","#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n","#    ds = ds.flat_map(lambda w: w.batch(window_size))\n","#    ds = ds.batch(32).prefetch(1)\n","#    forecast = model.predict(ds)\n","#    return forecast\n","\n","\n","#window_size = # YOUR CODE HERE\n","#rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n","#rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n","\n","#result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n","\n","## WE EXPECT AN MAE OF 15 or less for the maximum score\n","#score = ceil(20 - result)\n","#if score > 5:\n","#    score = 5\n","\n","#print(score)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n","Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n","Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.3.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.31.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.35.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.18.5)\n","Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.12.4)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (49.6.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.17.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.6.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.7.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d (Conv1D)              (None, None, 60)          360       \n","_________________________________________________________________\n","lstm (LSTM)                  (None, None, 60)          29040     \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, None, 60)          29040     \n","_________________________________________________________________\n","dropout (Dropout)            (None, None, 60)          0         \n","_________________________________________________________________\n","dense (Dense)                (None, None, 32)          1952      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, None, 16)          528       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, None, 1)           17        \n","_________________________________________________________________\n","lambda (Lambda)              (None, None, 1)           0         \n","=================================================================\n","Total params: 60,937\n","Trainable params: 60,937\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","     93/Unknown - 7s 80ms/step - loss: 38.3398 - mae: 38.8548\n","Epoch 00001: val_mae improved from inf to 23.37518, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 8s 88ms/step - loss: 38.3400 - mae: 38.8548 - val_loss: 22.8131 - val_mae: 23.3752\n","Epoch 2/100\n","92/93 [============================>.] - ETA: 0s - loss: 26.5478 - mae: 27.0429\n","Epoch 00002: val_mae improved from 23.37518 to 18.19161, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 42ms/step - loss: 26.6080 - mae: 27.0920 - val_loss: 17.4718 - val_mae: 18.1916\n","Epoch 3/100\n","92/93 [============================>.] - ETA: 0s - loss: 24.1441 - mae: 24.6386\n","Epoch 00003: val_mae improved from 18.19161 to 17.77797, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 45ms/step - loss: 24.1833 - mae: 24.6706 - val_loss: 17.3836 - val_mae: 17.7780\n","Epoch 4/100\n","91/93 [============================>.] - ETA: 0s - loss: 23.1579 - mae: 23.6524\n","Epoch 00004: val_mae did not improve from 17.77797\n","93/93 [==============================] - 4s 43ms/step - loss: 23.1836 - mae: 23.6716 - val_loss: 17.5164 - val_mae: 17.8603\n","Epoch 5/100\n","91/93 [============================>.] - ETA: 0s - loss: 22.7237 - mae: 23.2183\n","Epoch 00005: val_mae improved from 17.77797 to 16.23796, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 43ms/step - loss: 22.7506 - mae: 23.2393 - val_loss: 15.7756 - val_mae: 16.2380\n","Epoch 6/100\n","92/93 [============================>.] - ETA: 0s - loss: 21.7237 - mae: 22.2176\n","Epoch 00006: val_mae did not improve from 16.23796\n","93/93 [==============================] - 4s 47ms/step - loss: 21.6828 - mae: 22.1842 - val_loss: 16.5174 - val_mae: 16.7862\n","Epoch 7/100\n","92/93 [============================>.] - ETA: 0s - loss: 21.5287 - mae: 22.0227\n","Epoch 00007: val_mae did not improve from 16.23796\n","93/93 [==============================] - 4s 44ms/step - loss: 21.5575 - mae: 22.0462 - val_loss: 15.9438 - val_mae: 16.7184\n","Epoch 8/100\n","92/93 [============================>.] - ETA: 0s - loss: 21.4706 - mae: 21.9643\n","Epoch 00008: val_mae improved from 16.23796 to 15.64985, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 44ms/step - loss: 21.4478 - mae: 21.9457 - val_loss: 15.1471 - val_mae: 15.6498\n","Epoch 9/100\n","91/93 [============================>.] - ETA: 0s - loss: 20.7118 - mae: 21.2053\n","Epoch 00009: val_mae did not improve from 15.64985\n","93/93 [==============================] - 4s 44ms/step - loss: 20.7445 - mae: 21.2342 - val_loss: 16.1948 - val_mae: 16.7186\n","Epoch 10/100\n","92/93 [============================>.] - ETA: 0s - loss: 20.6724 - mae: 21.1658\n","Epoch 00010: val_mae did not improve from 15.64985\n","93/93 [==============================] - 4s 45ms/step - loss: 20.6577 - mae: 21.1538 - val_loss: 15.5317 - val_mae: 16.0660\n","Epoch 11/100\n","92/93 [============================>.] - ETA: 0s - loss: 20.5211 - mae: 21.0146\n","Epoch 00011: val_mae improved from 15.64985 to 14.92857, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 46ms/step - loss: 20.5135 - mae: 21.0084 - val_loss: 14.4780 - val_mae: 14.9286\n","Epoch 12/100\n","91/93 [============================>.] - ETA: 0s - loss: 20.2540 - mae: 20.7475\n","Epoch 00012: val_mae did not improve from 14.92857\n","93/93 [==============================] - 4s 47ms/step - loss: 20.2463 - mae: 20.7432 - val_loss: 14.5946 - val_mae: 15.2534\n","Epoch 13/100\n","92/93 [============================>.] - ETA: 0s - loss: 19.9789 - mae: 20.4721\n","Epoch 00013: val_mae did not improve from 14.92857\n","93/93 [==============================] - 4s 44ms/step - loss: 19.9918 - mae: 20.4826 - val_loss: 14.7472 - val_mae: 15.2058\n","Epoch 14/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.6763 - mae: 20.1692\n","Epoch 00014: val_mae did not improve from 14.92857\n","93/93 [==============================] - 4s 44ms/step - loss: 19.7233 - mae: 20.2119 - val_loss: 14.6008 - val_mae: 15.1253\n","Epoch 15/100\n","92/93 [============================>.] - ETA: 0s - loss: 19.7723 - mae: 20.2654\n","Epoch 00015: val_mae did not improve from 14.92857\n","93/93 [==============================] - 4s 44ms/step - loss: 19.7753 - mae: 20.2678 - val_loss: 15.3202 - val_mae: 15.8488\n","Epoch 16/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.6934 - mae: 20.1862\n","Epoch 00016: val_mae did not improve from 14.92857\n","93/93 [==============================] - 4s 43ms/step - loss: 19.7094 - mae: 20.2033 - val_loss: 16.0311 - val_mae: 16.3674\n","Epoch 17/100\n","92/93 [============================>.] - ETA: 0s - loss: 19.5227 - mae: 20.0159\n","Epoch 00017: val_mae did not improve from 14.92857\n","93/93 [==============================] - 4s 44ms/step - loss: 19.5517 - mae: 20.0396 - val_loss: 15.1146 - val_mae: 15.4304\n","Epoch 18/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.5858 - mae: 20.0788\n","Epoch 00018: val_mae did not improve from 14.92857\n","93/93 [==============================] - 4s 46ms/step - loss: 19.5764 - mae: 20.0668 - val_loss: 15.6936 - val_mae: 16.0320\n","Epoch 19/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.3843 - mae: 19.8770\n","Epoch 00019: val_mae improved from 14.92857 to 14.63062, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 44ms/step - loss: 19.4602 - mae: 19.9426 - val_loss: 14.1583 - val_mae: 14.6306\n","Epoch 20/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.2540 - mae: 19.7471\n","Epoch 00020: val_mae did not improve from 14.63062\n","93/93 [==============================] - 4s 45ms/step - loss: 19.2234 - mae: 19.7232 - val_loss: 14.3242 - val_mae: 14.8379\n","Epoch 21/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.2302 - mae: 19.7230\n","Epoch 00021: val_mae improved from 14.63062 to 14.33807, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 42ms/step - loss: 19.2638 - mae: 19.7547 - val_loss: 13.7486 - val_mae: 14.3381\n","Epoch 22/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.0281 - mae: 19.5211\n","Epoch 00022: val_mae did not improve from 14.33807\n","93/93 [==============================] - 4s 43ms/step - loss: 19.0160 - mae: 19.5090 - val_loss: 14.3267 - val_mae: 14.8084\n","Epoch 23/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.1262 - mae: 19.6191\n","Epoch 00023: val_mae did not improve from 14.33807\n","93/93 [==============================] - 4s 44ms/step - loss: 19.0989 - mae: 19.5953 - val_loss: 13.7830 - val_mae: 14.4753\n","Epoch 24/100\n","92/93 [============================>.] - ETA: 0s - loss: 19.1522 - mae: 19.6451\n","Epoch 00024: val_mae did not improve from 14.33807\n","93/93 [==============================] - 4s 44ms/step - loss: 19.1143 - mae: 19.6141 - val_loss: 13.6199 - val_mae: 14.4425\n","Epoch 25/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.8821 - mae: 19.3747\n","Epoch 00025: val_mae did not improve from 14.33807\n","93/93 [==============================] - 4s 43ms/step - loss: 18.8890 - mae: 19.3818 - val_loss: 14.4280 - val_mae: 14.7449\n","Epoch 26/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.9597 - mae: 19.4528\n","Epoch 00026: val_mae did not improve from 14.33807\n","93/93 [==============================] - 4s 44ms/step - loss: 18.9834 - mae: 19.4756 - val_loss: 13.8334 - val_mae: 14.3949\n","Epoch 27/100\n","92/93 [============================>.] - ETA: 0s - loss: 19.1041 - mae: 19.5970\n","Epoch 00027: val_mae did not improve from 14.33807\n","93/93 [==============================] - 4s 45ms/step - loss: 19.1100 - mae: 19.6018 - val_loss: 15.0033 - val_mae: 15.1087\n","Epoch 28/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.7991 - mae: 19.2914\n","Epoch 00028: val_mae did not improve from 14.33807\n","93/93 [==============================] - 5s 49ms/step - loss: 18.8190 - mae: 19.3077 - val_loss: 14.2670 - val_mae: 14.6843\n","Epoch 29/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.7133 - mae: 19.2056\n","Epoch 00029: val_mae improved from 14.33807 to 14.26859, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 46ms/step - loss: 18.7263 - mae: 19.2168 - val_loss: 13.6858 - val_mae: 14.2686\n","Epoch 30/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.6928 - mae: 19.1855\n","Epoch 00030: val_mae did not improve from 14.26859\n","93/93 [==============================] - 4s 45ms/step - loss: 18.6841 - mae: 19.1784 - val_loss: 13.9742 - val_mae: 14.3381\n","Epoch 31/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.6581 - mae: 19.1506\n","Epoch 00031: val_mae did not improve from 14.26859\n","93/93 [==============================] - 4s 46ms/step - loss: 18.6496 - mae: 19.1436 - val_loss: 13.9763 - val_mae: 14.3648\n","Epoch 32/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.7149 - mae: 19.2074\n","Epoch 00032: val_mae did not improve from 14.26859\n","93/93 [==============================] - 5s 48ms/step - loss: 18.6829 - mae: 19.1813 - val_loss: 14.5724 - val_mae: 14.9304\n","Epoch 33/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.5680 - mae: 19.0610\n","Epoch 00033: val_mae improved from 14.26859 to 14.20141, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 47ms/step - loss: 18.5401 - mae: 19.0383 - val_loss: 13.6945 - val_mae: 14.2014\n","Epoch 34/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.6982 - mae: 19.1908\n","Epoch 00034: val_mae did not improve from 14.20141\n","93/93 [==============================] - 4s 43ms/step - loss: 18.6718 - mae: 19.1616 - val_loss: 14.1468 - val_mae: 14.4591\n","Epoch 35/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.5249 - mae: 19.0174\n","Epoch 00035: val_mae did not improve from 14.20141\n","93/93 [==============================] - 4s 46ms/step - loss: 18.5382 - mae: 19.0262 - val_loss: 13.8548 - val_mae: 14.2351\n","Epoch 36/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.5527 - mae: 19.0452\n","Epoch 00036: val_mae did not improve from 14.20141\n","93/93 [==============================] - 4s 45ms/step - loss: 18.5681 - mae: 19.0574 - val_loss: 14.4126 - val_mae: 14.9520\n","Epoch 37/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.8391 - mae: 19.3318\n","Epoch 00037: val_mae did not improve from 14.20141\n","93/93 [==============================] - 4s 44ms/step - loss: 18.8149 - mae: 19.3067 - val_loss: 15.2250 - val_mae: 15.4127\n","Epoch 38/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.4264 - mae: 18.9190\n","Epoch 00038: val_mae did not improve from 14.20141\n","93/93 [==============================] - 4s 47ms/step - loss: 18.4280 - mae: 18.9204 - val_loss: 13.8109 - val_mae: 14.3423\n","Epoch 39/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.5425 - mae: 19.0351\n","Epoch 00039: val_mae did not improve from 14.20141\n","93/93 [==============================] - 4s 44ms/step - loss: 18.5319 - mae: 19.0265 - val_loss: 13.9323 - val_mae: 14.2548\n","Epoch 40/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.6631 - mae: 19.1559\n","Epoch 00040: val_mae did not improve from 14.20141\n","93/93 [==============================] - 4s 44ms/step - loss: 18.6934 - mae: 19.1799 - val_loss: 13.7813 - val_mae: 14.4629\n","Epoch 41/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.2595 - mae: 18.7517\n","Epoch 00041: val_mae improved from 14.20141 to 14.19624, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 47ms/step - loss: 18.2511 - mae: 18.7428 - val_loss: 13.6628 - val_mae: 14.1962\n","Epoch 42/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.2924 - mae: 18.7849\n","Epoch 00042: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 46ms/step - loss: 18.2868 - mae: 18.7821 - val_loss: 14.3432 - val_mae: 14.8679\n","Epoch 43/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.1998 - mae: 18.6918\n","Epoch 00043: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 47ms/step - loss: 18.2178 - mae: 18.7099 - val_loss: 13.6844 - val_mae: 14.3122\n","Epoch 44/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.3107 - mae: 18.8031\n","Epoch 00044: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 43ms/step - loss: 18.2937 - mae: 18.7892 - val_loss: 14.0769 - val_mae: 14.3340\n","Epoch 45/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.3959 - mae: 18.8884\n","Epoch 00045: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 44ms/step - loss: 18.3952 - mae: 18.8913 - val_loss: 15.2428 - val_mae: 15.6090\n","Epoch 46/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.3426 - mae: 18.8349\n","Epoch 00046: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 43ms/step - loss: 18.3692 - mae: 18.8566 - val_loss: 13.8528 - val_mae: 14.3485\n","Epoch 47/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.2595 - mae: 18.7520\n","Epoch 00047: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 47ms/step - loss: 18.2803 - mae: 18.7690 - val_loss: 13.6529 - val_mae: 14.2908\n","Epoch 48/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.1564 - mae: 18.6487\n","Epoch 00048: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 45ms/step - loss: 18.1507 - mae: 18.6435 - val_loss: 13.8270 - val_mae: 14.2816\n","Epoch 49/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.0790 - mae: 18.5714\n","Epoch 00049: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 48ms/step - loss: 18.0834 - mae: 18.5750 - val_loss: 13.7980 - val_mae: 14.1971\n","Epoch 50/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.1126 - mae: 18.6050\n","Epoch 00050: val_mae did not improve from 14.19624\n","93/93 [==============================] - 4s 42ms/step - loss: 18.1184 - mae: 18.6112 - val_loss: 13.8031 - val_mae: 14.2718\n","Epoch 51/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.0350 - mae: 18.5269\n","Epoch 00051: val_mae improved from 14.19624 to 14.10251, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 45ms/step - loss: 18.0582 - mae: 18.5490 - val_loss: 13.6283 - val_mae: 14.1025\n","Epoch 52/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.3066 - mae: 18.7988\n","Epoch 00052: val_mae did not improve from 14.10251\n","93/93 [==============================] - 4s 45ms/step - loss: 18.3427 - mae: 18.8303 - val_loss: 14.0075 - val_mae: 14.1800\n","Epoch 53/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.1625 - mae: 18.6548\n","Epoch 00053: val_mae did not improve from 14.10251\n","93/93 [==============================] - 4s 43ms/step - loss: 18.1884 - mae: 18.6759 - val_loss: 15.3095 - val_mae: 15.4459\n","Epoch 54/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.1449 - mae: 18.6374\n","Epoch 00054: val_mae did not improve from 14.10251\n","93/93 [==============================] - 4s 48ms/step - loss: 18.1564 - mae: 18.6472 - val_loss: 13.8203 - val_mae: 14.3923\n","Epoch 55/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.1365 - mae: 18.6283\n","Epoch 00055: val_mae did not improve from 14.10251\n","93/93 [==============================] - 4s 43ms/step - loss: 18.1194 - mae: 18.6143 - val_loss: 13.8454 - val_mae: 14.3998\n","Epoch 56/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.9728 - mae: 18.4651\n","Epoch 00056: val_mae did not improve from 14.10251\n","93/93 [==============================] - 5s 49ms/step - loss: 17.9997 - mae: 18.4879 - val_loss: 13.9098 - val_mae: 14.2404\n","Epoch 57/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.9588 - mae: 18.4510\n","Epoch 00057: val_mae did not improve from 14.10251\n","93/93 [==============================] - 4s 44ms/step - loss: 17.9885 - mae: 18.4772 - val_loss: 14.3566 - val_mae: 14.2814\n","Epoch 58/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.0241 - mae: 18.5167\n","Epoch 00058: val_mae improved from 14.10251 to 14.07872, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 4s 43ms/step - loss: 18.0304 - mae: 18.5218 - val_loss: 13.7007 - val_mae: 14.0787\n","Epoch 59/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.1388 - mae: 18.6311\n","Epoch 00059: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 18.1222 - mae: 18.6190 - val_loss: 13.7155 - val_mae: 14.4374\n","Epoch 60/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.1276 - mae: 18.6197\n","Epoch 00060: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 48ms/step - loss: 18.1041 - mae: 18.5967 - val_loss: 13.7774 - val_mae: 14.3332\n","Epoch 61/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.0422 - mae: 18.5343\n","Epoch 00061: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 46ms/step - loss: 18.0370 - mae: 18.5276 - val_loss: 13.6335 - val_mae: 14.2126\n","Epoch 62/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.9307 - mae: 18.4227\n","Epoch 00062: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 17.9174 - mae: 18.4118 - val_loss: 13.6928 - val_mae: 14.2915\n","Epoch 63/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.0135 - mae: 18.5054\n","Epoch 00063: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 18.0164 - mae: 18.5077 - val_loss: 13.8328 - val_mae: 14.2553\n","Epoch 64/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.0254 - mae: 18.5177\n","Epoch 00064: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 18.0562 - mae: 18.5456 - val_loss: 14.0036 - val_mae: 14.3779\n","Epoch 65/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.0730 - mae: 18.5653\n","Epoch 00065: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 46ms/step - loss: 18.0823 - mae: 18.5729 - val_loss: 13.8839 - val_mae: 14.6453\n","Epoch 66/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.9954 - mae: 18.4876\n","Epoch 00066: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 44ms/step - loss: 18.0025 - mae: 18.4934 - val_loss: 13.9206 - val_mae: 14.2195\n","Epoch 67/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.9794 - mae: 18.4715\n","Epoch 00067: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 17.9690 - mae: 18.4630 - val_loss: 13.5755 - val_mae: 14.2483\n","Epoch 68/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.1362 - mae: 18.6284\n","Epoch 00068: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 18.1292 - mae: 18.6232 - val_loss: 14.0188 - val_mae: 14.5047\n","Epoch 69/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.8102 - mae: 18.3021\n","Epoch 00069: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.8174 - mae: 18.3091 - val_loss: 13.9922 - val_mae: 14.2976\n","Epoch 70/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.9299 - mae: 18.4219\n","Epoch 00070: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.9375 - mae: 18.4281 - val_loss: 13.8752 - val_mae: 14.2864\n","Epoch 71/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.8471 - mae: 18.3390\n","Epoch 00071: val_mae did not improve from 14.07872\n","93/93 [==============================] - 5s 49ms/step - loss: 17.8608 - mae: 18.3502 - val_loss: 14.0527 - val_mae: 14.5078\n","Epoch 72/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.8289 - mae: 18.3210\n","Epoch 00072: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 44ms/step - loss: 17.8246 - mae: 18.3190 - val_loss: 13.9772 - val_mae: 14.3503\n","Epoch 73/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.8151 - mae: 18.3071\n","Epoch 00073: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 46ms/step - loss: 17.8184 - mae: 18.3058 - val_loss: 14.5019 - val_mae: 15.1194\n","Epoch 74/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.8163 - mae: 18.3081\n","Epoch 00074: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.8042 - mae: 18.2983 - val_loss: 14.0473 - val_mae: 14.5524\n","Epoch 75/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.7944 - mae: 18.2865\n","Epoch 00075: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 47ms/step - loss: 17.7940 - mae: 18.2865 - val_loss: 15.1056 - val_mae: 15.8169\n","Epoch 76/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.8544 - mae: 18.3465\n","Epoch 00076: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.8451 - mae: 18.3389 - val_loss: 13.7475 - val_mae: 14.2970\n","Epoch 77/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.7873 - mae: 18.2794\n","Epoch 00077: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.7604 - mae: 18.2575 - val_loss: 13.4750 - val_mae: 14.2319\n","Epoch 78/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.7966 - mae: 18.2886\n","Epoch 00078: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 44ms/step - loss: 17.8452 - mae: 18.3299 - val_loss: 13.7820 - val_mae: 14.2029\n","Epoch 79/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.7366 - mae: 18.2285\n","Epoch 00079: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 44ms/step - loss: 17.7476 - mae: 18.2375 - val_loss: 14.0549 - val_mae: 14.7618\n","Epoch 80/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.7257 - mae: 18.2177\n","Epoch 00080: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 17.7162 - mae: 18.2040 - val_loss: 13.7386 - val_mae: 14.2131\n","Epoch 81/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.6534 - mae: 18.1454\n","Epoch 00081: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 17.6659 - mae: 18.1554 - val_loss: 14.4287 - val_mae: 14.6013\n","Epoch 82/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.7097 - mae: 18.2015\n","Epoch 00082: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 46ms/step - loss: 17.7138 - mae: 18.2073 - val_loss: 14.4522 - val_mae: 14.9537\n","Epoch 83/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.7317 - mae: 18.2236\n","Epoch 00083: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 17.7550 - mae: 18.2425 - val_loss: 14.3469 - val_mae: 14.6768\n","Epoch 84/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.8055 - mae: 18.2976\n","Epoch 00084: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 44ms/step - loss: 17.8428 - mae: 18.3325 - val_loss: 14.0194 - val_mae: 14.4109\n","Epoch 85/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.8427 - mae: 18.3345\n","Epoch 00085: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 44ms/step - loss: 17.8450 - mae: 18.3364 - val_loss: 13.8529 - val_mae: 14.4044\n","Epoch 86/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.7601 - mae: 18.2522\n","Epoch 00086: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 46ms/step - loss: 17.7530 - mae: 18.2486 - val_loss: 15.0805 - val_mae: 15.7104\n","Epoch 87/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.7321 - mae: 18.2243\n","Epoch 00087: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.7240 - mae: 18.2173 - val_loss: 13.7859 - val_mae: 14.3343\n","Epoch 88/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.7921 - mae: 18.2840\n","Epoch 00088: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.8010 - mae: 18.2912 - val_loss: 14.5893 - val_mae: 14.7424\n","Epoch 89/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.6179 - mae: 18.1097\n","Epoch 00089: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 47ms/step - loss: 17.6423 - mae: 18.1321 - val_loss: 13.9156 - val_mae: 14.6048\n","Epoch 90/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.6878 - mae: 18.1800\n","Epoch 00090: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 46ms/step - loss: 17.6835 - mae: 18.1765 - val_loss: 14.0016 - val_mae: 14.3765\n","Epoch 91/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.5993 - mae: 18.0914\n","Epoch 00091: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 48ms/step - loss: 17.6386 - mae: 18.1262 - val_loss: 14.1788 - val_mae: 14.6623\n","Epoch 92/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.7744 - mae: 18.2665\n","Epoch 00092: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 17.7852 - mae: 18.2747 - val_loss: 14.6134 - val_mae: 15.2284\n","Epoch 93/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.6984 - mae: 18.1905\n","Epoch 00093: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 17.7513 - mae: 18.2387 - val_loss: 14.2280 - val_mae: 14.5246\n","Epoch 94/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.6559 - mae: 18.1478\n","Epoch 00094: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.6426 - mae: 18.1372 - val_loss: 13.8693 - val_mae: 14.2728\n","Epoch 95/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.6384 - mae: 18.1304\n","Epoch 00095: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.6671 - mae: 18.1538 - val_loss: 13.8537 - val_mae: 14.2771\n","Epoch 96/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.5551 - mae: 18.0468\n","Epoch 00096: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 43ms/step - loss: 17.5610 - mae: 18.0500 - val_loss: 13.9507 - val_mae: 14.3540\n","Epoch 97/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.5379 - mae: 18.0298\n","Epoch 00097: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.5608 - mae: 18.0485 - val_loss: 14.1015 - val_mae: 14.5060\n","Epoch 98/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.8010 - mae: 18.2933\n","Epoch 00098: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 45ms/step - loss: 17.8041 - mae: 18.2958 - val_loss: 13.5577 - val_mae: 14.1379\n","Epoch 99/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.5887 - mae: 18.0806\n","Epoch 00099: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 46ms/step - loss: 17.6141 - mae: 18.1014 - val_loss: 14.0304 - val_mae: 14.4456\n","Epoch 100/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.5497 - mae: 18.0415\n","Epoch 00100: val_mae did not improve from 14.07872\n","93/93 [==============================] - 4s 44ms/step - loss: 17.5559 - mae: 18.0466 - val_loss: 14.1520 - val_mae: 14.4906\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3FtFba2gOcsg","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}