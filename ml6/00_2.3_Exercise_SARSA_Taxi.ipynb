{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_2.3_Exercise_SARSA_Taxi.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPzsvrJJYfu/oqaRz8f9F+9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F5tABggTVKQw","colab_type":"text"},"source":["# SARSA & Q - Learning\n","\n","# 연습데이터 : Taxi\n","![좋은거](https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png)\n","\n","**학습목표**\n","\n","1. SARSA의 업데이트 맛 본다.\n","2. Q-learning의 업데이트를 맛 본다.\n","3. 일단 코드 짤 수 있다."]},{"cell_type":"markdown","metadata":{"id":"uJB96tIRZfeT","colab_type":"text"},"source":["# 필요 라이브러리 불러오기\n","\n","1. 이런 연습에서는 딱히 비디오 영상이 필요하지 않다."]},{"cell_type":"code","metadata":{"id":"Bi_Z3_kaZRot","colab_type":"code","colab":{}},"source":["!pip install gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVEK7ArIZoeN","colab_type":"code","colab":{}},"source":["import numpy as np\n","import gym"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttRBaI2WZqcX","colab_type":"text"},"source":["# 사용할 환경 불러오기\n","[택시!](https://gym.openai.com/envs/Taxi-v3/)\n"]},{"cell_type":"code","metadata":{"id":"86nb4VeAaACM","colab_type":"code","colab":{}},"source":["env = gym.make('Taxi-v3')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrgrAKT09lsD","colab_type":"text"},"source":["# SARSA와, Q-learning 준비 해보기\n","\n","**핵심 코드**\n","1. SARSA : S,A,R,S,A 가 끝나면 Q업데이트!\n","```\n","Q[s,a] = Q[s,a] + alpha * ((r + gama* Q[s1,a1]) - Q[s,a])\n","```\n","2. Q-Learning : S,A,R,S 후 max A, 그다음 Q업데이트!\n","```\n","Q[s,a] = Q[s,a] + alpha*(r + gama*np.max(Q[s1,:]) - Q[s,a])\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"sq0VaqTqfrW7","colab_type":"text"},"source":["# I. SARSA 연습"]},{"cell_type":"code","metadata":{"id":"O1SPcVpcgNG9","colab_type":"code","colab":{}},"source":["alpha = 0.2\n","gamma = 0.999 # 할인율!\n","n_episod = 20000\n","epsilon = 0.15"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MNpjAB66gfj5","colab_type":"code","colab":{}},"source":["Q = ## Q-table 작성\n","\n","rewards_sarsa = []\n","\n","for i in range(n_episod) : \n","    s0 = env.reset()\n","    done = False\n","    a0 =           # 처음엔 랜덤무브\n","\n","    while True :\n","        ## a0를 이용해 움직이고\n","        ## s1, r1, done, _를 return 받는 코드를 짜자.\n","                                      # step!\n","\n","        # e-greedy 방법을 구현해두자.\n","        # 행동을 선택하여 a1에 선언하자.\n","\n","\n","\n","\n","        # Update 코드를 작성해보자.\n","        # ( s0, a0, r1, s1, a1 ) 로 부터!\n","\n","\n","\n","\n","        if done == True: # 종료되었으면\n","            rewards_sarsa.append(r1)\n","            env.close() # 환경 닫고.\n","            break # 멈춰야지\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","        a0 = a1 # 다음 루프에선 이것이 직전 action\n","\n","    if (i+1) % 4000 == 0 :\n","        print('===========  에피소드 : {}  ============'.format(i+1))\n","        # for rows in Q.reshape([-1,4,4]):\n","        #     print(*rows)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ktMvL2nSva8V","colab_type":"text"},"source":["### 학습 완료된 것 구경\n","### 무한 루프 주의\n","위 에서, 학습을 1 ~ 2회만 시켜보고 아래를 해봐도 좋다."]},{"cell_type":"code","metadata":{"id":"QrVEfKVtvSo7","colab_type":"code","colab":{}},"source":["s0 = env.reset()\n","done = False\n","# 처음엔 랜덤액션 선택하여 a0에 선언\n","a0 = #####\n","#\n","while True :\n","    env.render()\n","    # a0를 이용, 환경과 상호작용하여  s1, r1, done, _를 받아온다.\n","\n","    greedy_actions = np.argwhere(Q[s1, :] == np.amax(Q[s1, :])).reshape(-1)\n","    a1 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","    # 무한 루프 걸린다면 위의 두줄 주석, 아래를 주석 해제\n","    # if np.random.uniform() < epsilon :  # 무슨 뜻?\n","    #     a1 = env.action_space.sample()\n","    # else : \n","    #     greedy_actions = np.argwhere(Q[s1, :] == np.amax(Q[s1, :])).reshape(-1)\n","    #     a1 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","\n","    if done == True: # 종료되었으면\n","        env.render()\n","        env.close() # 환경 닫고.\n","        break # 멈춰야지\n","\n","    s0 = s1 # 다음 루프에선 이것이 직전 state\n","    a0 = a1 # 다음 루프에선 이것이 직전 action"],"execution_count":null,"outputs":[]}]}