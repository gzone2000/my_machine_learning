{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_1.6_MC_Q_exercise.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOUq/+XvVIVSLzfLuErd8Io"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3UzaouW5VdZu","colab_type":"text"},"source":["# Monte Carlo Method : Action - Value estimation\n","\n","**공부할 내용**\n","1. state, action, reward 가 무엇인가요?\n","2. Return이 무엇인가요?\n","3. State-Value Function 이란?\n","4. Action-Value Function 이란?\n","5. Policy 란?\n","\n","**사용할 환경 : 자주 만날 Frozen Lake**\n","\n","![좋은거](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSKrspq5EvLOC2Rv8dfsw1OVqD1WjN5YsmgAHVCVn3x8zLhM5Y8&usqp=CAU)\n"]},{"cell_type":"markdown","metadata":{"id":"hJU-vI1RdB_L","colab_type":"text"},"source":["# 필요 라이브러리 불러오기."]},{"cell_type":"code","metadata":{"id":"Cby2BD-qdKCS","colab_type":"code","colab":{}},"source":["!pip install gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXHT6L_cdSPJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9wZyfzXdTRr","colab_type":"code","colab":{}},"source":["# gym.envs.registration.register(\n","#     id=\"FrozenLake-v3\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","#     kwargs={'map_name': '4x4', 'is_slippery': False}\n","# )\n","\n","gym.envs.registration.register(\n","    id=\"FrozenLake-v8\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name': '8x8', 'is_slippery': True}\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOoQHPoRdhjY","colab_type":"code","colab":{}},"source":["# env = gym.make('FrozenLake-v3')\n","env = gym.make('FrozenLake-v8')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAS1RV3JnqsN","colab_type":"text"},"source":["## Total 연습!\n","\n","1. 총 10000번의 episode를 시행한다!\n","    1. 하나의 episode가 진행 중일 땐,\n","        * states = [] # state 기록!\n","        * actions = [] # action 기록\n","        * rewards = [] # reward 기록!\n","    2. 하나의 episode가 종료되면\n","        * returns = [] # $G_t$ 제작, 기록!\n","        * (사실 이 단계에서 $V(s)$를 업데이트 해야 좋음 - episode마다 업데이트가 되니까!)\n","2. episode가 끝나면\n","    * sa_n : state-action별 방문횟수 업데이트 하는 넘파이 어레이 제작\n","    * Q : state-action별 가치값이 저장될 0으로 가득찬 2차원 array제작\n","    * states, sa_n, returns 이용하여 V업데이트!\n","\n","$$Q(S_t, A_t) = Q(S_t, A_t) + {1\\over{N(S_t,A_t)}}(G_t - Q(S_t, A_t))$$"]},{"cell_type":"code","metadata":{"id":"mlQJSYHwjw7_","colab_type":"code","colab":{}},"source":["####### Your Code Here ##########\n","\n","states = []\n","actions = []\n","returns = []\n","\n","gamma = 0.9\n","\n","for i in range(10000) :\n","    # 환경을 초기화 하며 초기 state 를 s0에 선언하시오.\n","    # rewards 와 temp_returns 에 빈 리스트를 선언하시오.\n","\n","\n","    while True :\n","        # 랜덤한 액션을 하나 선택하여 a0에 담으시오.\n","\n","        # a0를 이용하여 환경과 상호작용 하면서, s1, r1, done, _ 를 선언하시오.\n"," \n","        \n","        # states에 s0을 담고 rewards에 r1을 담으시오.\n","        # a0 도 actions에 담으시오.\n","\n","\n","\n","\n","        if done == True :\n","            env.close() # 환경닫기\n","            break\n","        # 다음 단계의 s0에 s1의 값을 옮겨 선언하시오.\n","\n","\n","\n","    ### 이 episode에 대해서 Return을 계산하여 temp_returns에 담고\n","    ### returns = returns + temp_returns\n","    ### temp_returns.append()아님 주의! insert!\n","    Gt = 0\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-je478DytNnN","colab_type":"code","colab":{}},"source":["len(states), len(returns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"na6ipqIBtRay","colab_type":"code","colab":{}},"source":["sa_n =     # state-action 쌍이 등장한 횟수를 카운트할 2차원 어레이\n","Q =    # state-action의 가치를 기록할 2차원 어레이\n","#state, returns, actions 이용하여 Q를 업데이트 하자.\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63imBPbhtVOE","colab_type":"text"},"source":["# 자! Action-Value 바탕으로 움직인다면?\n","\n","바꿔 말해, Policy를 제작한다면?\n","\n","$$\\pi'(s) = argmax_{a}(Q(s,a) )$$"]},{"cell_type":"code","metadata":{"id":"P5DHy9401_81","colab_type":"code","colab":{}},"source":["print(Q[4])\n","print(Q[4].argmax())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSj2XTIK2IV8","colab_type":"code","colab":{}},"source":["s0 = env.reset()\n","env.render()\n","for i in range(1000):\n","    a0 = Q[s0].argmax()\n","    s1, r1, done, _ = env.step(a0)\n","    env.render()\n","    if done == True :\n","        env.close()\n","        break\n","    s0 = s1"],"execution_count":0,"outputs":[]}]}