{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_2.2_Exercise_QL_FL8.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPWjZDNkNP9Br1Ve63Xe9Dv"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F5tABggTVKQw","colab_type":"text"},"source":["# SARSA & Q - Learning\n","\n","# 연습데이터 : Frozen Lake\n","![좋은거](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSqnklD5CDrIqDgS28YCAcmQkfBhJVvYirrRbZjmZeta5Dfx4_e&usqp=CAU)\n","\n","**학습목표**\n","\n","1. SARSA의 업데이트 맛 본다.\n","2. Q-learning의 업데이트를 맛 본다.\n","3. 일단 코드 짤 수 있다."]},{"cell_type":"markdown","metadata":{"id":"uJB96tIRZfeT","colab_type":"text"},"source":["# 필요 라이브러리 불러오기\n","\n","1. 이런 연습에서는 딱히 비디오 영상이 필요하지 않다."]},{"cell_type":"code","metadata":{"id":"Bi_Z3_kaZRot","colab_type":"code","colab":{}},"source":["!pip install gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVEK7ArIZoeN","colab_type":"code","colab":{}},"source":["import numpy as np\n","import gym"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttRBaI2WZqcX","colab_type":"text"},"source":["# 사용할 환경 불러오기"]},{"cell_type":"code","metadata":{"id":"LzYjwZKRZsCV","colab_type":"code","colab":{}},"source":["# gym.envs.registration.register(\n","#     id=\"FrozenLake-v3\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","#     kwargs={'map_name': '4x4', 'is_slippery': False}\n","# )\n","\n","gym.envs.registration.register(\n","    id=\"FrozenLake-v8\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name': '8x8', 'is_slippery': True}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86nb4VeAaACM","colab_type":"code","colab":{}},"source":["# env = gym.make('FrozenLake-v3')\n","env = gym.make('FrozenLake-v8')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrgrAKT09lsD","colab_type":"text"},"source":["# SARSA와, Q-learning 준비 해보기\n","\n","**핵심 코드**\n","1. SARSA : S,A,R,S,A 가 끝나면 Q업데이트!\n","```\n","Q[s,a] = Q[s,a] + alpha * ((r + gama* Q[s1,a1]) - Q[s,a])\n","```\n","2. Q-Learning : S,A,R,S 후 max A, 그다음 Q업데이트!\n","```\n","Q[s,a] = Q[s,a] + alpha*(r + gama*np.max(Q[s1,:]) - Q[s,a])\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"epY6RCn6vxTi","colab_type":"text"},"source":["# II. Q-Learning\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7QCkWqYYwFz1"},"source":["### 1. Q-Table을 준비하자.\n","\n","* Q = np.zeros([state의 수, action의 수])\n","* 활용 예시\n","    * Q[2, 3]에는 10이 담겨있음.\n","    * Q[2, 1]에는 5가 담겨있음.\n","    * 2번 state에선 3번 액션이 1번 액션보다 2배 가치 있음!\n","\n","* env.action_space.n, env.observation_space.n 이용"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"luOV_L62wFz3","colab":{}},"source":["env.observation_space, env.action_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pn5AS4K9wFz6","colab":{}},"source":["## Your Code Here\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6UaQusebwFz8","colab":{}},"source":["print(Q)\n","# for rows in Q.reshape([4,4,4]):\n","#     print(*rows)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tJAykWHJwFz-"},"source":["### 2. parameter들"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AQ3qf8YSwFz-","colab":{}},"source":["alpha = 0.2\n","gamma = 0.999 # 할인율!\n","n_episod = 20000\n","epsilon = 0.15"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JQgSONUJwF0A"},"source":["### 3. q-learning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8aoMzJ0IwF0B","colab":{}},"source":["Q = np.zeros([env.observation_space.n,\n","              env.action_space.n])\n","\n","rewards_ql = []\n","\n","for i in range(n_episod) : \n","    s0 = env.reset()\n","    done = False\n","\n","    while True :\n","        # e-greedy 하게 행동을 선택하여 a0에 선언하자.\n"," \n"," \n","\n","\n","        # a0 를 이용하여 행동을 하고\n","        # s1, r1, done, _ 를 새로 선언하다.\n","\n","\n","        # ( s0, a0, r1, s1 ) 로 부터! 차이점 주의!\n","        # 업데이트 코드를 작성하자.\n","\n","\n","\n","\n","        if done == True: # 종료되었으면\n","            rewards_ql.append(r1)\n","            env.close() # 환경 닫고.\n","            break # 멈춰야지\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","\n","    if (i+1) % 4000 == 0 :\n","        print('===========  에피소드 : {}  ============'.format(i+1))\n","        # for rows in Q.reshape([-1,4,4]):\n","        #     print(*rows)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ImORkklkwF0D"},"source":["### 학습 완료된 것 구경\n","\n","위 에서, 학습을 1 ~ 2회만 시켜보고 아래를 해봐도 좋다."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4-BKSI37wF0E","colab":{}},"source":["s0 = env.reset()\n","done = False\n","a0 = env.action_space.sample() # 처음엔 랜덤무브\n","\n","while True :\n","    env.render()\n","    s1, r1, done, _ = env.step(a0) # 일단 움직인다.\n","\n","    greedy_actions = np.argwhere(Q[s1, :] == np.amax(Q[s1, :])).reshape(-1)\n","    a1 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","    if done == True: # 종료되었으면\n","        env.render()\n","        env.close() # 환경 닫고.\n","        break # 멈춰야지\n","\n","    s0 = s1 # 다음 루프에선 이것이 직전 state\n","    a0 = a1 # 다음 루프에선 이것이 직전 action"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTjjfmS6wGoC","colab_type":"text"},"source":["### Q-learning 학습 코드를 보고 진행 순서를 정리하자."]},{"cell_type":"code","metadata":{"id":"E5TaTRXZwM6C","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}