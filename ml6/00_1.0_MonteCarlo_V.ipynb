{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_1.5_MonteCarlo_V.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN4lDLgnu/svG20++cullJR"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3UzaouW5VdZu","colab_type":"text"},"source":["# Monte Carlo Method : State - Value estimation\n","\n","**공부할 내용**\n","1. state, action, reward 가 무엇인가요?\n","2. Return이 무엇인가요?\n","3. State-Value Function 이란?\n","4. Action-Value Function 이란?\n","5. Policy 란?\n","\n","**사용할 환경 : 자주 만날 Frozen Lake**\n","\n","![좋은거](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSKrspq5EvLOC2Rv8dfsw1OVqD1WjN5YsmgAHVCVn3x8zLhM5Y8&usqp=CAU)\n"]},{"cell_type":"markdown","metadata":{"id":"hJU-vI1RdB_L","colab_type":"text"},"source":["# 필요 라이브러리 불러오기."]},{"cell_type":"code","metadata":{"id":"Cby2BD-qdKCS","colab_type":"code","colab":{}},"source":["!pip install gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXHT6L_cdSPJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9wZyfzXdTRr","colab_type":"code","colab":{}},"source":["gym.envs.registration.register(\n","    id=\"FrozenLake-v3\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name': '4x4', 'is_slippery': False}\n",")\n","\n","# gym.envs.registration.register(\n","#     id=\"FrozenLake-v8\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","#     kwargs={'map_name': '8x8', 'is_slippery': True}\n","# )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOoQHPoRdhjY","colab_type":"code","colab":{}},"source":["env = gym.make('FrozenLake-v3')\n","# env = gym.make('FrozenLake-v8')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7k5VAB9Rdi6g","colab_type":"text"},"source":["# 일단, 랜덤하게 진행한 한 번의 Episode를 기록해볼까?!"]},{"cell_type":"code","metadata":{"id":"jA8Z6uIHdsQy","colab_type":"code","colab":{}},"source":["states = []\n","rewards = []\n","\n","s0 = env.reset()\n","env.render()\n","while True :\n","    a0 = env.action_space.sample()\n","    s1, r1, done, _ = env.step(a0) # 움직여!\n","\n","    states.append(s0)\n","    rewards.append(r1)\n","    env.render()\n","\n","    if done == True :\n","        env.close() # 환경닫기\n","        break\n","\n","    s0 = s1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2A-IklwPfFWn","colab_type":"code","colab":{}},"source":["states"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-Zbb4a9fNh4","colab_type":"code","colab":{}},"source":["rewards"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LC8E04adfUf_","colab_type":"text"},"source":["# Return 만들기 연습!"]},{"cell_type":"code","metadata":{"id":"XXGhszcvfXBv","colab_type":"code","colab":{}},"source":["Returns = [  ]\n","rewards_sim = [1,2,7,-1,2,-4,5,2,2,-100]\n","gamma = 0.9\n","\n","Gt = 0\n","for reward in rewards_sim[::-1]:\n","    print(reward)\n","    #### Your Code Here ####\n","    Gt = reward + gamma*Gt\n","    Returns.insert(0, Gt)\n","\n","print(Returns)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j6CgmOI-gbMD","colab_type":"text"},"source":["# State 별로 가치를 관찰해보자."]},{"cell_type":"code","metadata":{"id":"G0YTEkiVgtdW","colab_type":"code","colab":{}},"source":["sim_returns = Returns[:len(states)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eht525gqgeHV","colab_type":"code","colab":{}},"source":["len(states), len(sim_returns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkjRaCpmgnMM","colab_type":"code","colab":{}},"source":["print(states)\n","print(sim_returns)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzsGVEd1hFms","colab_type":"text"},"source":["# State-Value (V)를 일단 제작해보자.\n","\n","V[3] 이라고 한다면, 3번 state의 가치가 나와야 한다!"]},{"cell_type":"code","metadata":{"id":"JUbmfShKhoXz","colab_type":"code","colab":{}},"source":["print(env.observation_space)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVp0AgCZhekR","colab_type":"code","colab":{}},"source":["V = np.zeros(shape=(16,))\n","V"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mu28BtZshkur","colab_type":"text"},"source":["### 쉬운 여정"]},{"cell_type":"code","metadata":{"id":"yYclFUY-h8TS","colab_type":"code","colab":{}},"source":["V_dict = {k:[] for k in range(16)}\n","V_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsFob9keiJlZ","colab_type":"code","colab":{}},"source":["for idx,state in enumerate(states):\n","    V_dict[state].append(sim_returns[idx])\n","V_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGMA3LXqjDiU","colab_type":"code","colab":{}},"source":["for state, values in V_dict.items() :\n","    if len(values) == 0:\n","        continue\n","    V[state] = np.mean(values)\n","V.reshape([4,4])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDCXT39IjvYp","colab_type":"text"},"source":["# 조오금 고급진 여정"]},{"cell_type":"code","metadata":{"id":"CM7SHJzljySj","colab_type":"code","colab":{}},"source":["state_n = np.zeros(shape=(16,))\n","V = np.zeros(shape=(16,))\n","#state, sim_returns 이용하는 것은 맞다.\n","for t, state in enumerate(states):\n","    state_n[state] = state_n[state] + 1\n","    n_s = state_n[state]\n","    V[state] = V[state] + (1/n_s)*(sim_returns[t] - V[state])\n","\n","V.reshape([4,4])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAS1RV3JnqsN","colab_type":"text"},"source":["## Total 연습!\n","\n","1. 총 10000번의 episode를 시행한다!\n","    1. 하나의 episode가 진행 중일 땐,\n","        * states = [] # state 기록!\n","        * rewards = [] # reward 기록!\n","    2. 하나의 episode가 종료되면\n","        * returns = [] # $G_t$ 제작, 기록!\n","        * (사실 이 단계에서 $V(s)$를 업데이트 해야 좋음 - episode마다 업데이트가 되니까!)\n","2. episode가 끝나면\n","    * state_n : state별 방문횟수 업데이트 하는 넘파이 어레이 제작\n","    * V : state별 가치값이 저장될 0으로 가득찬 array제작\n","    * states, state_n, returns 이용하여 V업데이트!\n","\n","$$V(S_t) = V(S_t) + {1\\over{N(S_t)}}(G_t - V(S_t))$$"]},{"cell_type":"code","metadata":{"id":"mlQJSYHwjw7_","colab_type":"code","colab":{}},"source":["####### Your Code Here ##########\n","\n","states = []\n","returns = []\n","\n","gamma = 0.9\n","\n","for i in range(10000) :\n","    # 환경을 초기화 하며 초기 state 를 s0에 선언하시오.\n","\n","    rewards = []\n","    temp_returns = []\n","    while True :\n","        # 랜덤한 액션을 하나 선택하여 a0에 담으시오.\n","\n","\n","        # 환경과 상호작용 하면서, s1, r1, done, _ 를 선언하시오.\n","                   = env.step(   )\n","        \n","        # states에 s0을 담고 rewards에 r1을 담으시오.\n","\n","\n","\n","        if done == True :\n","            env.close() # 환경닫기\n","            break\n","        # 다음 단계의 s0에 s1의 값을 옮겨 선언하시오.\n","        s0 = s1\n","\n","    ### 이 episode에 대해서 Return을 계산하여 temp_returns에 담고\n","    ### returns = returns + temp_returns\n","    ### temp_returns.append()아님 주의! insert!\n","    Gt = 0\n","    for r in rewards[::-1] : \n","        ### Gt 를 업데이트 하는 코드\n","        \n","        temp_returns.insert(0, Gt)\n","\n","    returns = returns + temp_returns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-je478DytNnN","colab_type":"code","colab":{}},"source":["len(states), len(returns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"na6ipqIBtRay","colab_type":"code","colab":{}},"source":["state_n = np.zeros(shape=(16,))\n","V = np.zeros(shape=(16,))\n","#state, returns 이용할 것\n","for t, state in enumerate(states):\n","    state_n[state] = state_n[state] + 1\n","    n_s = state_n[state]\n","   # V 를 업데이트 하는 코드\n","\n","V.reshape([4,4])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1qqCfRF390J","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","def plot_values(V):\n","\t# reshape value function\n","\tV_sq = np.reshape(V, (4,4))\n","\n","\t# plot the state-value function\n","\tfig = plt.figure(figsize=(8, 8))\n","\tax = fig.add_subplot(111)\n","\tim = ax.imshow(V_sq, cmap='cool')\n","\tfor (j,i),label in np.ndenumerate(V_sq):\n","\t    ax.text(i, j, np.round(label, 5), ha='center', va='center', fontsize=14)\n","\tplt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n","\tplt.title('State-Value Function')\n","\tplt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SjATAdKN3-a2","colab_type":"code","colab":{}},"source":["plot_values(V)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63imBPbhtVOE","colab_type":"text"},"source":["# 자! State-Value 바탕으로 움직인다면?\n","\n","바꿔 말해, Policy를 제작한다면?\n","\n","$$\\pi'(s) = argmax_{a}(R^a_s + P^a_{ss'}V(s') )$$"]}]}