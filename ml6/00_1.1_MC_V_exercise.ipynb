{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_1.5_MC_V_exercise.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNhfEojuXflqxy0uYIifloz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3UzaouW5VdZu","colab_type":"text"},"source":["# Monte Carlo Method : State - Value estimation\n","\n","**공부할 내용**\n","1. state, action, reward 가 무엇인가요?\n","2. Return이 무엇인가요?\n","3. State-Value Function 이란?\n","4. Action-Value Function 이란?\n","5. Policy 란?\n","\n","**사용할 환경 : 자주 만날 Frozen Lake**\n","\n","![좋은거](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSKrspq5EvLOC2Rv8dfsw1OVqD1WjN5YsmgAHVCVn3x8zLhM5Y8&usqp=CAU)\n"]},{"cell_type":"markdown","metadata":{"id":"hJU-vI1RdB_L","colab_type":"text"},"source":["# 필요 라이브러리 불러오기."]},{"cell_type":"code","metadata":{"id":"Cby2BD-qdKCS","colab_type":"code","colab":{}},"source":["!pip install gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXHT6L_cdSPJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9wZyfzXdTRr","colab_type":"code","colab":{}},"source":["# gym.envs.registration.register(\n","#     id=\"FrozenLake-v3\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","#     kwargs={'map_name': '4x4', 'is_slippery': False}\n","# )\n","\n","gym.envs.registration.register(\n","    id=\"FrozenLake-v8\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name': '8x8', 'is_slippery': True}\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOoQHPoRdhjY","colab_type":"code","colab":{}},"source":["# env = gym.make('FrozenLake-v3')\n","env = gym.make('FrozenLake-v8')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAS1RV3JnqsN","colab_type":"text"},"source":["## Total 연습!\n","\n","1. 총 10000번의 episode를 시행한다! ( 결과를 보고 2만번으로 늘려서도 진행해본다)\n","    1. 하나의 episode가 진행 중일 땐,\n","        * states = [] # state 기록!\n","        * rewards = [] # reward 기록!\n","    2. 하나의 episode가 종료되면\n","        * returns = [] # $G_t$ 제작, 기록!\n","        * (사실 이 단계에서 $V(s)$를 업데이트 해야 좋음 - episode마다 업데이트가 되니까!)\n","2. episode가 끝나면\n","    * state_n : state별 방문횟수 업데이트 하는 넘파이 어레이 제작\n","    * V : state별 가치값이 저장될 0으로 가득찬 array제작\n","    * states, state_n, returns 이용하여 V업데이트!\n","\n","$$V(S_t) = V(S_t) + {1\\over{N(S_t)}}(G_t - V(S_t))$$"]},{"cell_type":"code","metadata":{"id":"mlQJSYHwjw7_","colab_type":"code","colab":{}},"source":["%%time\n","####### Your Code Here ##########\n","\n","states = []\n","returns = []\n","\n","gamma = 0.9\n","\n","for i in range(20000) :\n","    # 환경을 초기화 하며 초기 state 를 s0에 선언하시오.\n","    # rewards 와 temp_returns 에 list를 만들어두시오.\n","\n","    rewards = []\n","    temp_returns = []\n","    while True :\n","        # 랜덤한 액션을 하나 선택하여 a0에 담으시오.\n","\n","        # 환경과 상호작용 하면서, s1, r1, done, _ 를 선언하시오.\n","\n","        \n","        # states에 s0을 담고 rewards에 r1을 담으시오.\n","\n","\n","\n","        if done == True :\n","        # 환경닫기\n","            break\n","        # 다음 단계의 s0에 s1의 값을 옮겨 선언하시오.\n","        s0 = s1\n","\n","    ### 이 episode에 대해서, rewards를 이용하여 Return을 계산하여 temp_returns에 담고\n","    ### temp_returns.append()아님 주의! insert!\n","    ### returns = returns + temp_returns\n","    Gt = 0\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-je478DytNnN","colab_type":"code","colab":{}},"source":["len(states), len(returns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ed_O2E0vxHXf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"na6ipqIBtRay","colab_type":"code","colab":{}},"source":["state_n =    # state의 방문횟수가 담길 1차원 어레이 제작. 0으로 가득 차 있어야 함.\n","V =  # State의 가치가 담길 1차원 어레이 제작. 0으로 일단 가득차있어야 함.\n","#state, returns 이용 하여 V를 제작하자!\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s2MubMdyxpa0","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","def plot_values(V):\n","\t# reshape value function\n","\tV_sq = np.reshape(V, (8,8))\n","\n","\t# plot the state-value function\n","\tfig = plt.figure(figsize=(8, 8))\n","\tax = fig.add_subplot(111)\n","\tim = ax.imshow(V_sq, cmap='cool')\n","\tfor (j,i),label in np.ndenumerate(V_sq):\n","\t    ax.text(i, j, np.round(label, 5), ha='center', va='center', fontsize=14)\n","\tplt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n","\tplt.title('State-Value Function')\n","\tplt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6vziVMrxp-s","colab_type":"code","colab":{}},"source":["plot_values(V)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63imBPbhtVOE","colab_type":"text"},"source":["# 자! State-Value 바탕으로 움직인다면?\n","\n","바꿔 말해, Policy를 제작한다면?\n","\n","$$\\pi'(s) = argmax_{a}(R^a_s + P^a_{ss'}V(s') )$$"]}]}