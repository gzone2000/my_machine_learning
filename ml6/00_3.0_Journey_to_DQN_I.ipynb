{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_3_Journey_to_DQN_I.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPF0Iwiqq4LNNfk4rJ7TvGi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZCY7LwMxFN9V","colab_type":"text"},"source":["# Deep Q Network로의 여행.\n","# Part I. Function Approximation : Q-Network\n","\n","![좋은 그림](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnature14236/MediaObjects/41586_2015_Article_BFnature14236_Fig1_HTML.jpg)\n","\n","[읽어보면 좋은 것 1](https://arxiv.org/pdf/1312.5602.pdf)<br>\n","[읽어보면 좋은 것 2](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xklO0e3y-5nl","colab_type":"text"},"source":["# 라이브러리 설치 / 불러오기"]},{"cell_type":"code","metadata":{"id":"NJXIDTmawqn6","colab_type":"code","colab":{}},"source":["%%time\n","## 약 25초 ~30초 소요\n","!pip install pyvirtualdisplay \n","!apt-get install -y xvfb python-opengl ffmpeg\n","!pip install gym\n","!pip install box2d-py\n","#!pip install pyglet==1.3.2\n","!pip install pyglet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmOw7kOJ-0ht","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4JtH_2M-3YY","colab_type":"code","colab":{}},"source":["display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGlgV8N3-4Qj","colab_type":"text"},"source":["비디오 녹화용 함수"]},{"cell_type":"code","metadata":{"id":"XuIFZVbo--K_","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[-1]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sArxiosg--cU","colab_type":"text"},"source":["# LunarLander-v2"]},{"cell_type":"code","metadata":{"id":"lYyFQyA9_Fys","colab_type":"code","colab":{}},"source":["env = wrap_env(gym.make(\"LunarLander-v2\"))\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)\n","\n","state = env.reset()\n","for t in range(1000):\n","    action = env.action_space.sample() # your agent here (this takes random actions)\n","    env.render()\n","    observation, reward, done, info = env.step(action)\n","    if done: \n","      break;\n","            \n","print('steps: ', t)\n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRgflNf4_Lgg","colab_type":"text"},"source":["# Deep Neural Network for Q-function\n","\n","**Q-function기능을 할 뉴럴넷을 구성할 것이다.**\n","1. input은 state다. (노드 수는?)\n","2. output은 그 state에서 취할 수 있는 action에 대한 Q값이다. (노드 수는?)\n","\n","![좋은그림](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/11/20/Fig2-DeepRL-SageMaker.gif)"]},{"cell_type":"code","metadata":{"id":"3YuvdNAp_87B","colab_type":"code","colab":{}},"source":["print(\"state 수는? : \", env.observation_space.shape)\n","print(\"action 수는? : \", env.action_space.n)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tv9r6_CXBCUB","colab_type":"text"},"source":["**다음과 같이 구성하시오.**\n","1. Q값에 대한 회귀 문제이다! (아웃풋레이어의 activation은?)\n","2. 히든레이어는 2개를 구성한다.(각각 노드 64개씩)\n","3. 컴파일 까지!\n","4. **Functional**하게!  (너무 힘들면 Sequential하게)"]},{"cell_type":"code","metadata":{"id":"ekpE4onvBihd","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3v8zZtzSBTcV","colab_type":"code","colab":{}},"source":["keras.backend.clear_session()\n","######################\n","### Your Code Here ###\n","######################\n","\n","state_layer = \n","\n","\n","\n","q_layer = \n","\n","Q_network = Model(state_layer, q_layer)\n","\n","Q_network.compile(loss = 'mse',\n","              optimizer = Adam())\n","\n","Q_network.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7f5syqDCFqD","colab_type":"text"},"source":["# Q - Learning\n","\n","**00_2_SARSA & Qlearning 파일 참고하며 볼 것!**\n","\n","1. 흐름을 정리하며 봐야 가장 좋다!"]},{"cell_type":"code","metadata":{"id":"d4AD7VQYEMXA","colab_type":"code","colab":{}},"source":["env.close()\n","\n","alpha = 0.1\n","gamma = 0.999\n","n_episod = 2000\n","epsilon = 0.1\n","\n","cum_rewards = []\n","\n","for i in range(n_episod) :\n","    print(\"episode {} --진행 중\".format(i+1))\n","    env = wrap_env(gym.make(\"LunarLander-v2\"))    \n","    s0 = env.reset()\n","    s0 = s0.reshape([1, -1]) # 2차원 어레이로 바꿔주기\n","    done = False\n","    cum_r = 0\n","    while True :\n","        Q_s0 = Q_network.predict(s0) #s0에서의 action들의 Q_value\n","        # 행동 선택하기 e-greedy 방법\n","        if np.random.uniform() < epsilon :\n","            a0 = env.action_space.sample()\n","        else : \n","            a0 = np.argmax(Q_s0)\n","\n","        # 환경과 상호작용!\n","        s1, r1, done, _ = env.step(a0)\n","        s1 = s1.reshape([1,-1]) # 2차원 어레이로 바꿔주기\n","\n","        # update Q 제작! 엄청 중요!\n","        # Q_s0[a0] 만 업데이트가 일어나도록 update용 Q_value를 제작한다.\n","        Q_s1 = Q_network.predict(s1)\n","        update_Q = Q_s0.copy() # Q_s0와 다 똑같지만\n","        update_Q[0][a0] = r1 + gamma*np.max(Q_s1)*(1-done) # 딱 Q[s0, a0] 자리에 이 값을 넣어준다.\n","        # 1-done의 의미를 탐구해보자.\n","        ###### Q-table에서 업데이트는 ? #############################################\n","        ## Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*np.max(Q[s1,:]) - Q[s0, a0]) #\n","        #############################################################################\n","\n","        Q_network.fit(s0, update_Q, epochs=1, verbose=0)\n","        \n","        cum_r = cum_r + r1\n","\n","        if done == True : # 종료 되었다면\n","            cum_rewards.append(cum_r)\n","            env.close() # 환경닫고\n","            break # 멈추자.\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","\n","    if (i+1) % 4 == 0 :\n","            print('===========  에피소드 : {}  ============'.format(i+1))\n","            print('최종 누적 보상 :',cum_r)\n","            print(a0, Q_s0)\n","            plt.plot(cum_rewards)\n","            plt.show()\n","            show_video()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"of4rToNCIhK_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}