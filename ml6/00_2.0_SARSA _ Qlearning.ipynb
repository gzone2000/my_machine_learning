{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_2_SARSA _ Qlearning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfTWmhJfvq1rSknJpeaLfA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F5tABggTVKQw","colab_type":"text"},"source":["# SARSA & Q - Learning\n","\n","# 연습데이터 : Frozen Lake\n","![좋은거](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSKrspq5EvLOC2Rv8dfsw1OVqD1WjN5YsmgAHVCVn3x8zLhM5Y8&usqp=CAU)\n","\n","**학습목표**\n","\n","1. SARSA의 업데이트 맛 본다.\n","2. Q-learning의 업데이트를 맛 본다.\n","3. 일단 코드 짤 수 있다."]},{"cell_type":"markdown","metadata":{"id":"NBlQZsBgXA4x","colab_type":"text"},"source":["# 미리 알아야 했지만..\n","\n","[미리 알면 참 좋은 것. 1](https://image.slidesharecdn.com/reinforcementlearning-170904070210/95/introduction-of-deep-reinforcement-learning-22-638.jpg?cb=1504578048)\n","\n","[역시 알아야 참 좋은 것. 2](http://www.modulabs.co.kr/files/attach/images/334/237/003/18eba72dcfeafa6e6280055a95078ffa.png)"]},{"cell_type":"markdown","metadata":{"id":"uJB96tIRZfeT","colab_type":"text"},"source":["# 필요 라이브러리 불러오기\n","\n","1. 이런 연습에서는 딱히 비디오 영상이 필요하지 않다."]},{"cell_type":"code","metadata":{"id":"Bi_Z3_kaZRot","colab_type":"code","colab":{}},"source":["!pip install gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVEK7ArIZoeN","colab_type":"code","colab":{}},"source":["import numpy as np\n","import gym"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttRBaI2WZqcX","colab_type":"text"},"source":["# 사용할 환경 불러오기"]},{"cell_type":"code","metadata":{"id":"LzYjwZKRZsCV","colab_type":"code","colab":{}},"source":["gym.envs.registration.register(\n","    id=\"FrozenLake-v3\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name': '4x4', 'is_slippery': False}\n",")\n","\n","# gym.envs.registration.register(\n","#     id=\"FrozenLake-v8\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","#     kwargs={'map_name': '8x8', 'is_slippery': True}\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86nb4VeAaACM","colab_type":"code","colab":{}},"source":["env = gym.make('FrozenLake-v3')\n","# env = gym.make('FrozenLake-v8')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrgrAKT09lsD","colab_type":"text"},"source":["# SARSA와, Q-learning 준비 해보기\n","\n","**핵심 코드**\n","1. SARSA : S,A,R,S,A 가 끝나면 Q업데이트!\n","```\n","Q[s,a] = Q[s,a] + alpha * ((r + gama* Q[s1,a1]) - Q[s,a])\n","```\n","2. Q-Learning : S,A,R,S 후 max A, 그다음 Q업데이트!\n","```\n","Q[s,a] = Q[s,a] + alpha*(r + gama*np.max(Q[s1,:]) - Q[s,a])\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"sq0VaqTqfrW7","colab_type":"text"},"source":["# I. SARSA"]},{"cell_type":"markdown","metadata":{"id":"nfORhpDsayLQ","colab_type":"text"},"source":["### 1. Q-Table을 준비하자.\n","\n","* Q = np.zeros([state의 수, action의 수])\n","* 활용 예시\n","    * Q[2, 3]에는 10이 담겨있음.\n","    * Q[2, 1]에는 5가 담겨있음.\n","    * 2번 state에선 3번 액션이 1번 액션보다 2배 가치 있음!\n","\n","* env.action_space.n, env.observation_space.n 이용"]},{"cell_type":"code","metadata":{"id":"5Z1Q9xpMeoVK","colab_type":"code","colab":{}},"source":["## Your Code Here\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4tstupmfLux","colab_type":"code","colab":{}},"source":["print(Q)\n","# for rows in Q.reshape([4,4,4]):\n","#     print(*rows)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"722t9WpVfpEp","colab_type":"text"},"source":["### 2. parameter들"]},{"cell_type":"code","metadata":{"id":"O1SPcVpcgNG9","colab_type":"code","colab":{}},"source":["alpha = 0.2\n","gamma = 0.999 # 할인율!\n","n_episod = 20000\n","epsilon = 0.15"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2gpuoQfgdbH","colab_type":"text"},"source":["### 3. SARSA!"]},{"cell_type":"code","metadata":{"id":"MNpjAB66gfj5","colab_type":"code","colab":{}},"source":["Q = np.zeros([env.observation_space.n,\n","              env.action_space.n])\n","\n","rewards_sarsa = []\n","\n","for i in range(n_episod) : \n","    s0 = env.reset()\n","    done = False\n","    a0 = env.action_space.sample() # 처음엔 랜덤무브\n","\n","    while True :\n","        s1, r1, done, _ = env.step(a0) # 일단 움직인다.\n","\n","        if np.random.uniform() < epsilon :  # 무슨 뜻?\n","            a1 = env.action_space.sample()\n","        else : \n","            greedy_actions = np.argwhere(Q[s1, :] == np.amax(Q[s1, :])).reshape(-1)\n","            a1 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","        # Update 한다!\n","        # ( s0, a0, r1, s1, a1 ) 로 부터!\n","        \n","        Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*Q[s1, a1] - Q[s0, a0])\n","\n","        if done == True: # 종료되었으면\n","            rewards_sarsa.append(r1)\n","            env.close() # 환경 닫고.\n","            break # 멈춰야지\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","        a0 = a1 # 다음 루프에선 이것이 직전 action\n","\n","    if (i+1) % 4000 == 0 :\n","        print('===========  에피소드 : {}  ============'.format(i+1))\n","        for rows in Q.reshape([4,4,4]):\n","            print(*rows)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ktMvL2nSva8V","colab_type":"text"},"source":["### 학습 완료된 것 구경\n","### 무한 루프 주의\n","위 에서, 학습을 1 ~ 2회만 시켜보고 아래를 해봐도 좋다."]},{"cell_type":"code","metadata":{"id":"QrVEfKVtvSo7","colab_type":"code","colab":{}},"source":["s0 = env.reset()\n","done = False\n","a0 = env.action_space.sample() # 처음엔 랜덤무브\n","\n","while True :\n","    env.render()\n","    s1, r1, done, _ = env.step(a0) # 일단 움직인다.\n","\n","    greedy_actions = np.argwhere(Q[s1, :] == np.amax(Q[s1, :])).reshape(-1)\n","    a1 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","    # 무한 루프 걸린다면 위의 두줄 주석, 아래를 주석 해제\n","    # if np.random.uniform() < epsilon :  # 무슨 뜻?\n","    #     a1 = env.action_space.sample()\n","    # else : \n","    #     greedy_actions = np.argwhere(Q[s1, :] == np.amax(Q[s1, :])).reshape(-1)\n","    #     a1 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","\n","    if done == True: # 종료되었으면\n","        env.render()\n","        env.close() # 환경 닫고.\n","        break # 멈춰야지\n","\n","    s0 = s1 # 다음 루프에선 이것이 직전 state\n","    a0 = a1 # 다음 루프에선 이것이 직전 action"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cphzXgO-v1c_","colab_type":"text"},"source":["### SARSA 학습코드를 보고, 진행 순서를 정리해보자."]},{"cell_type":"code","metadata":{"id":"On5jsUxHv-sA","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"epY6RCn6vxTi","colab_type":"text"},"source":["# II. Q-Learning\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7QCkWqYYwFz1"},"source":["### 1. Q-Table을 준비하자.\n","\n","* Q = np.zeros([state의 수, action의 수])\n","* 활용 예시\n","    * Q[2, 3]에는 10이 담겨있음.\n","    * Q[2, 1]에는 5가 담겨있음.\n","    * 2번 state에선 3번 액션이 1번 액션보다 2배 가치 있음!\n","\n","* env.action_space.n, env.observation_space.n 이용"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"luOV_L62wFz3","colab":{}},"source":["env.observation_space, env.action_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pn5AS4K9wFz6","colab":{}},"source":["## Your Code Here\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6UaQusebwFz8","colab":{}},"source":["print(Q)\n","# for rows in Q.reshape([4,4,4]):\n","#     print(*rows)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tJAykWHJwFz-"},"source":["### 2. parameter들"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AQ3qf8YSwFz-","colab":{}},"source":["alpha = 0.2\n","gamma = 0.999 # 할인율!\n","n_episod = 20000\n","epsilon = 0.15"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JQgSONUJwF0A"},"source":["### 3. q-learning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8aoMzJ0IwF0B","colab":{}},"source":["Q = np.zeros([env.observation_space.n,\n","              env.action_space.n])\n","\n","rewards_ql = []\n","\n","for i in range(n_episod) : \n","    s0 = env.reset()\n","    done = False\n","\n","    while True :\n","        # 행동을 선택\n","        if np.random.uniform() < epsilon :  # 무슨 뜻?\n","            a0 = env.action_space.sample()\n","        else : \n","            greedy_actions = np.argwhere(Q[s0, :] == np.amax(Q[s0, :])).reshape(-1)\n","            a0 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","        # a0 를 이용하여, 환경과 상호작용하고, s1, r1, done, _를 선언하시오.\n","        s1, r1, done, _ = env.step(a0)\n","        \n","        # ( s0, a0, r1, s1 ) 로 부터! 차이점 주의!\n","        Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*np.max(Q[s1,:]) - Q[s0, a0])\n","\n","        if done == True: # 종료되었으면\n","            rewards_ql.append(r1)\n","            env.close() # 환경 닫고.\n","            break # 멈춰야지\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","\n","    if (i+1) % 4000 == 0 :\n","        print('===========  에피소드 : {}  ============'.format(i+1))\n","        for rows in Q.reshape([4,4,4]):\n","            print(*rows)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ImORkklkwF0D"},"source":["### 학습 완료된 것 구경\n","\n","위 에서, 학습을 1 ~ 2회만 시켜보고 아래를 해봐도 좋다."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4-BKSI37wF0E","colab":{}},"source":["s0 = env.reset()\n","done = False\n","a0 = env.action_space.sample() # 처음엔 랜덤무브\n","\n","while True :\n","    env.render()\n","    s1, r1, done, _ = env.step(a0) # 일단 움직인다.\n","\n","    greedy_actions = np.argwhere(Q[s1, :] == np.amax(Q[s1, :])).reshape(-1)\n","    a1 = np.random.choice(greedy_actions) # 무슨 뜻?\n","\n","    if done == True: # 종료되었으면\n","        env.render()\n","        env.close() # 환경 닫고.\n","        break # 멈춰야지\n","\n","    s0 = s1 # 다음 루프에선 이것이 직전 state\n","    a0 = a1 # 다음 루프에선 이것이 직전 action"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTjjfmS6wGoC","colab_type":"text"},"source":["### Q-learning 학습 코드를 보고 진행 순서를 정리하자."]},{"cell_type":"code","metadata":{"id":"E5TaTRXZwM6C","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8-fYIoxxyPif","colab_type":"text"},"source":["# Bonus 시각화 :"]},{"cell_type":"code","metadata":{"id":"QmTyTtRByQ-_","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrZqVaZazTmA","colab_type":"code","colab":{}},"source":["win_ratio_s = []\n","win_ratio_ql = []\n","\n","sliding_windows = 100\n","for i in range(n_episod - 100 + 1) :\n","    wr_s = sum(rewards_sarsa[i:i+sliding_windows])/sliding_windows\n","    wr_q = sum(rewards_ql[i:i+sliding_windows])/sliding_windows\n","    win_ratio_s.append(wr_s)\n","    win_ratio_ql.append(wr_q)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRkUtaIsyThU","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(12,6))\n","plt.plot(win_ratio_s, label='SARSA')\n","plt.plot(win_ratio_ql, label='Q-Learning')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGbLLLqjykUE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}