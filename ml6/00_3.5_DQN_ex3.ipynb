{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_3_DQN_ex3.0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNnZpd6QkrFhM9oEV6DoqRA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZCY7LwMxFN9V","colab_type":"text"},"source":["# Deep Q Network로의 여행 연습\n","# Part III. Double & Dueling\n","\n","![좋은 그림](https://d3i71xaburhd42.cloudfront.net/e6a1640c03c50a55ef3e00a0592dbb0851fe33bb/3-Figure1-1.png)\n","\n","[읽어보면 좋은 것 0](https://www.katnoria.com/static/e59c56013a5d82a0ae94d9413076dfc0/1e043/dqn_algo.png)<br>\n","[읽어보면 좋은 것 1](https://arxiv.org/pdf/1312.5602.pdf)<br>\n","[읽어보면 좋은 것 2](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xklO0e3y-5nl","colab_type":"text"},"source":["# 라이브러리 설치 / 불러오기"]},{"cell_type":"code","metadata":{"id":"NJXIDTmawqn6","colab_type":"code","colab":{}},"source":["%%time\n","## 약 25초 ~30초 소요\n","!pip install pyvirtualdisplay \n","!apt-get install -y xvfb python-opengl ffmpeg\n","!pip install gym\n","!pip install box2d-py\n","#!pip install pyglet==1.3.2\n","!pip install pyglet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmOw7kOJ-0ht","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4JtH_2M-3YY","colab_type":"code","colab":{}},"source":["display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGlgV8N3-4Qj","colab_type":"text"},"source":["비디오 녹화용 함수"]},{"cell_type":"code","metadata":{"id":"XuIFZVbo--K_","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[-1]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sArxiosg--cU","colab_type":"text"},"source":["# CartPole"]},{"cell_type":"code","metadata":{"id":"lYyFQyA9_Fys","colab_type":"code","colab":{}},"source":["env = wrap_env(gym.make(\"CartPole-v1\"))\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)\n","\n","state = env.reset()\n","for t in range(1000):\n","    action = env.action_space.sample() # your agent here (this takes random actions)\n","    env.render()\n","    observation, reward, done, info = env.step(action)\n","    if done: \n","      break;\n","            \n","print('steps: ', t)\n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRgflNf4_Lgg","colab_type":"text"},"source":["# Deep Neural Network for Q-function\n","\n","**Q-function기능을 할 뉴럴넷을 구성할 것이다.**\n","1. input은 state다. (노드 수는?)\n","2. output은 그 state에서 취할 수 있는 action에 대한 Q값이다. (노드 수는?)\n","\n","[좋은그림](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9911AF455B9A0A9706)<br>\n","[좋은그림1](https://i.ytimg.com/vi/2-zGCx4iv_k/hqdefault.jpg)\n","![좋은그림1](https://theaisummer.com/assets/img/posts/Taking_Deep_Q_Networks_a_step_further/DDQN.jpg)\n"]},{"cell_type":"code","metadata":{"id":"3YuvdNAp_87B","colab_type":"code","colab":{}},"source":["print(\"state 수는? : \", env.observation_space.shape)\n","print(\"action 수는? : \", env.action_space.n)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekpE4onvBihd","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Input, Dense, Add, Subtract, Average, Lambda\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras.utils import plot_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-hcSe2lVvYz","colab_type":"code","colab":{}},"source":["### 이것은 일단 그냥 사용해보자!\n","### huber 로스!\n","from tensorflow.keras.losses import Huber\n","def mean_huber_loss(y_true, y_pred, clip_delta=1.0):\n","    return tf.keras.backend.mean(Huber(clip_delta)(y_true, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3v8zZtzSBTcV","colab_type":"code","colab":{}},"source":["keras.backend.clear_session()\n","######################\n","### Your Code Here ###\n","######################\n","\n","state_layer = Input(shape=(4,))\n","hidden = Dense(32, activation='relu')(state_layer)\n","hv = Dense(24, activation='relu')(hidden)\n","v_layer = Dense(1, name='V')(hv)\n","ha = Dense(24, activation='relu')(hidden)\n","a_layer = Dense(2)(ha)\n","q_layer = Add(name=\"Q\")([v_layer, a_layer]) #Broadcast 해준다\n","\n","\n","\n","Q_network = Model(state_layer, q_layer)\n","\n","Q_network.compile(loss = mean_huber_loss,\n","              optimizer = Adam(0.01))\n","\n","# Q_network.summary()\n","\n","plot_model(Q_network, show_shapes=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KOT30gwQccQ","colab_type":"text"},"source":["### 위 코드를 보고 target_Q_network를 구성하시오.\n","\n","* 똑같으면 된다.\n","* 단, 모델 이름은 target_Q_network"]},{"cell_type":"code","metadata":{"id":"F5MgtWdIQf8c","colab_type":"code","colab":{}},"source":["# keras.backend.clear_session()\n","######################\n","### Your Code Here ###\n","######################\n","\n","\n","\n","\n","\n","\n","target_Q_network = Model(state_layer, q_layer)\n","\n","target_Q_network.compile(loss = mean_huber_loss,\n","              optimizer = Adam(0.01))\n","\n","# Q_network.summary()\n","\n","plot_model(target_Q_network, show_shapes=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGT1J7CHQf-z","colab_type":"code","colab":{}},"source":["## Target Q 네트워크의 가중치는 같아야 한다.\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7f5syqDCFqD","colab_type":"text"},"source":["# Memory 구현 & Memory에 어느정도 Experience를 담아두기!\n","\n","1. (s0, a0, r1, s1, done)을 담아두면 된다. 튜플!\n","2. deque를 이용하여 최근 십만개의 Experience만 담아둘수 있도록 한다.\n","3. Experience 를 replay하며 배울 때는..\n","    * sample_size = bach_size = 128 개씩 경험을 랜덤추출하여 .fit()할 것이다!\n","    * 따라서 experience(s0, a0, r1, s1, done)은 충분히 미리 담아두자.\n","    * 랜덤액션으로 담아두어도 좋다."]},{"cell_type":"code","metadata":{"id":"cktYnNxYlarr","colab_type":"code","colab":{}},"source":["from collections import deque\n","memory = deque(maxlen = 20000)  # 리스트 처럼 사용이 가능하다."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKFhfBBanC-n","colab_type":"code","colab":{}},"source":["alpha = 0.1\n","gamma = 0.99\n","n_episod = 200\n","epsilon = 0.1\n","pre_play = 100 # 실제 담기는건 10개가 아닐 것!\n","\n","for i in range(pre_play) :\n","    print(i, \" 번째 에피소드,\")\n","    s0 = env.reset()\n","    s0 = s0.reshape([1, -1]) # 2차원 어레이로 바꿔주기\n","    done = False\n","\n","    while True :\n","\n","        a0 = env.action_space.sample() # 랜덤 액션!\n","        # 환경과 상호작용!\n","        s1, r1, done, _ = env.step(a0)\n","        s1 = s1.reshape([1,-1]) # 2차원 어레이로 바꿔주기\n","\n","        if done == False :\n","            memory.append( (s0, a0, r1, s1, done) )\n","            s0 = s1\n","        else :\n","            s1 = np.zeros(s0.shape)  # 끝나면 s1가 0이 됨!\n","            memory.append( (s0, a0, r1, s1, done) )\n","            env.close()\n","            break\n","    print(\"저장된 experience : {}\".format(len(memory)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpr7DtQAwe2r","colab_type":"text"},"source":["# Memory로 부터 배치 사이즈 만큼 데이터를 떼어 오는 함수 제작\n","\n","* .fit(s0, target_Q) 임을 고려하자\n","* 심지어 이제 진짜 target_Q_network 를 이용하여 만들어야 하는것이 맞다.\n"]},{"cell_type":"code","metadata":{"id":"qH-Pkyzl1AEr","colab_type":"code","colab":{}},"source":["import random\n","def create_batch(target_model, memory, gamma, batch_size = 128):\n","    sample = np.array(random.sample(memory, batch_size))\n","\n","    s0 = sample[:, 0]\n","    a0 = sample[:, 1].astype(np.int8)\n","    r1 = sample[:, 2]\n","    s1 = sample[:, 3]\n","    d = sample[:, 4]\n","\n","    s0_batch = np.vstack(s0)\n","    s1_batch = np.vstack(s1)\n","\n","    target_Q_batch = target_model.predict(s0_batch)\n","    Q_s1 = target_model.predict(s1_batch)\n","    ###### Q-table에서 업데이트는 ? #############################################\n","    ## Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*np.max(Q[s1,:]) - Q[s0, a0]) #\n","    #############################################################################\n","\n","    target_Q_batch[np.arange(batch_size),a0] =  #### 업데이트 식 적기\n","\n","    return s0_batch, target_Q_batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dilUhdpunC2Z","colab_type":"text"},"source":["# Memory로 부터 학습 하는 Q-Network!"]},{"cell_type":"code","metadata":{"id":"d4AD7VQYEMXA","colab_type":"code","colab":{}},"source":["env.close()\n","\n","alpha = 0.1\n","gamma = 0.99\n","n_episod = 2000\n","\n","epsilon = 0.2\n","\n","sample_size = batch_size = 128\n","cum_rewards = []\n","tq_update = 4 ## target_Q는 언제 업데이트 할래?\n","\n","for i in range(n_episod) :\n","    print(\"episode {} --진행 중\".format(i+1))\n","    env = wrap_env(gym.make(\"CartPole-v1\"))\n","    s0 = env.reset()\n","    s0 = s0.reshape([1, -1]) # 2차원 어레이로 바꿔주기\n","    done = False\n","\n","    cum_r = 0\n","    time_step = 0\n","    while True :\n","        Q_s0 = Q_network.predict(s0) #s0에서의 action들의 Q_value\n","\n","        epsilon = max(0.1, epsilon*0.999)\n","        # a0 행동 선택하기 e-greedy 방법\n","     \n","\n","        # a0로 환경과 상호작용! s1 r1 done _\n","        s\n","        s1 = s1.reshape([1,-1]) # 2차원 어레이로 바꿔주기\n","\n","        # 메모리에 저장!  (s0,a0,r1,s1,done)\n","        memory.append(     )\n","\n","        # 학습을 위해 Experience Replay!\n","        # create_batch 조심.\n","        s0_batch, target_Q_batch = create_batch(target_Q_network, memory, gamma, batch_size=batch_size) \n","        Q_network.fit(s0_batch, target_Q_batch, epochs=0, verbose=1, batch_size=batch_size)\n","\n","        cum_r = cum_r + r1\n","\n","        time_step = time_step + 1\n","        if time_step % tq_update == 0 :\n","            ## Target Q 네트워크의 가중치 업데이트\n","            target_Q_network.set_weights(Q_network.get_weights())\n","\n","        if done == True : # 종료 되었다면\n","            cum_rewards.append(cum_r)\n","            env.close() # 환경닫고\n","            ## Target Q 네트워크의 가중치 업데이트\n","            target_Q_network.set_weights(Q_network.get_weights())\n","            break # 멈추자.\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","\n","    if (i+1) % 10 == 0 :\n","        print('===========  에피소드 : {}  ============'.format(i+1))\n","        print('최종누적보상 :',cum_r)\n","        print(a0, Q_s0)\n","        plt.plot(cum_rewards)\n","        plt.show()\n","        show_video()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBd7FsHsg0Bj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}