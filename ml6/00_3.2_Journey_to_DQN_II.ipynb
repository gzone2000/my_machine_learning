{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_3_Journey_to_DQN_II.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBFyMG/Ue52EccQP40bvr/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZCY7LwMxFN9V","colab_type":"text"},"source":["# Deep Q Network로의 여행.\n","# Part II. Experience replay\n","\n","![좋은 그림](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnature14236/MediaObjects/41586_2015_Article_BFnature14236_Fig1_HTML.jpg)\n","\n","[읽어보면 좋은 것 1](https://arxiv.org/pdf/1312.5602.pdf)<br>\n","[읽어보면 좋은 것 2](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xklO0e3y-5nl","colab_type":"text"},"source":["# 라이브러리 설치 / 불러오기"]},{"cell_type":"code","metadata":{"id":"NJXIDTmawqn6","colab_type":"code","colab":{}},"source":["%%time\n","## 약 25초 ~30초 소요\n","!pip install pyvirtualdisplay \n","!apt-get install -y xvfb python-opengl ffmpeg\n","!pip install gym\n","!pip install box2d-py\n","#!pip install pyglet==1.3.2\n","!pip install pyglet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmOw7kOJ-0ht","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4JtH_2M-3YY","colab_type":"code","colab":{}},"source":["display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGlgV8N3-4Qj","colab_type":"text"},"source":["비디오 녹화용 함수"]},{"cell_type":"code","metadata":{"id":"XuIFZVbo--K_","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[-1]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sArxiosg--cU","colab_type":"text"},"source":["# LunarLander-v2"]},{"cell_type":"code","metadata":{"id":"lYyFQyA9_Fys","colab_type":"code","colab":{}},"source":["env = wrap_env(gym.make(\"LunarLander-v2\"))\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)\n","\n","state = env.reset()\n","for t in range(1000):\n","    action = env.action_space.sample() # your agent here (this takes random actions)\n","    env.render()\n","    observation, reward, done, info = env.step(action)\n","    if done: \n","      break;\n","            \n","print('steps: ', t)\n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRgflNf4_Lgg","colab_type":"text"},"source":["# Deep Neural Network for Q-function\n","\n","**Q-function기능을 할 뉴럴넷을 구성할 것이다.**\n","1. input은 state다. (노드 수는?)\n","2. output은 그 state에서 취할 수 있는 action에 대한 Q값이다. (노드 수는?)\n","\n","![좋은그림](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/11/20/Fig2-DeepRL-SageMaker.gif)"]},{"cell_type":"code","metadata":{"id":"3YuvdNAp_87B","colab_type":"code","colab":{}},"source":["print(\"state 수는? : \", env.observation_space.shape)\n","print(\"action 수는? : \", env.action_space.n)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tv9r6_CXBCUB","colab_type":"text"},"source":["**다음과 같이 구성하시오.**\n","1. Q값에 대한 회귀 문제이다! (아웃풋레이어의 activation은?)\n","2. 히든레이어는 2개를 구성한다.(각각 노드 64개씩)\n","3. 컴파일 까지!\n","4. **Functional**하게!  (너무 힘들면 Sequential하게)"]},{"cell_type":"code","metadata":{"id":"ekpE4onvBihd","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-hcSe2lVvYz","colab_type":"code","colab":{}},"source":["### 이것은 일단 그냥 사용해보자!\n","### huber 로스!\n","from tensorflow.keras.losses import Huber\n","def mean_huber_loss(y_true, y_pred, clip_delta=1.0):\n","    return tf.keras.backend.mean(Huber(clip_delta)(y_true, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3v8zZtzSBTcV","colab_type":"code","colab":{}},"source":["keras.backend.clear_session()\n","######################\n","### Your Code Here ###\n","######################\n","\n","state_layer = \n","\n","\n","\n","q_layer = \n","\n","Q_network = Model(state_layer, q_layer)\n","\n","Q_network.compile(loss = mean_huber_loss,\n","              optimizer = Adam())\n","\n","Q_network.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7f5syqDCFqD","colab_type":"text"},"source":["# Memory 구현 & Memory에 어느정도 Experience를 담아두기!\n","\n","1. (s0, a0, r1, s1, done)을 담아두면 된다. 튜플!\n","2. deque를 이용하여 최근 만개의 Experience만 담아둘수 있도록 한다.\n","3. Experience 를 replay하며 배울 때는..\n","    * sample_size = bach_size = 128 개씩 경험을 랜덤추출하여 .fit()할 것이다!\n","    * 따라서 experience(s0, a0, r1, s1, done)은 충분히 미리 담아두자.\n","    * 랜덤액션으로 담아두어도 좋다."]},{"cell_type":"code","metadata":{"id":"cktYnNxYlarr","colab_type":"code","colab":{}},"source":["from collections import deque\n","memory = deque(maxlen = 10000)  # 리스트 처럼 사용이 가능하다."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKFhfBBanC-n","colab_type":"code","colab":{}},"source":["alpha = 0.1\n","gamma = 0.95\n","n_episod = 100\n","epsilon = 0.1\n","pre_play = 100 # 실제 담기는건 10개가 아닐 것!\n","\n","for i in range(pre_play) :\n","    print(i, \" 번째 에피소드,\")\n","    s0 = env.reset()\n","    s0 = s0.reshape([1, -1]) # 2차원 어레이로 바꿔주기\n","    done = False\n","\n","    while True :\n","\n","        a0 = env.action_space.sample() # 랜덤 액션!\n","        # 환경과 상호작용!\n","        s1, r1, done, _ = env.step(a0)\n","        s1 = s1.reshape([1,-1]) # 2차원 어레이로 바꿔주기\n","\n","        if done == False :\n","            memory.append( (s0, a0, r1, s1, done) )\n","            s0 = s1\n","        else :\n","            s1 = np.zeros(s0.shape)  # 끝나면 s1가 0이 됨!\n","            memory.append( (s0, a0, r1, s1, done) )\n","            env.close()\n","            break\n","    print(\"저장된 experience : {}\".format(len(memory)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpr7DtQAwe2r","colab_type":"text"},"source":["# Memory로 부터 배치 사이즈 만큼 데이터를 떼어 오는 함수 제작\n","\n","* .fit(s0, target_Q) 임을 고려하자"]},{"cell_type":"code","metadata":{"id":"qH-Pkyzl1AEr","colab_type":"code","colab":{}},"source":["import random\n","def create_batch(model, memory, gamma, batch_size = 128):\n","    sample = np.array(random.sample(memory, batch_size))\n","\n","    s0 = sample[:, 0]\n","    a0 = sample[:, 1].astype(np.int8)\n","    r1 = sample[:, 2]\n","    s1 = sample[:, 3]\n","    d = sample[:, 4]\n","\n","    s0_batch = np.vstack(s0)\n","    s1_batch = np.vstack(s1)\n","    target_Q_batch = model.predict(s0_batch)\n","    Q_s1 = model.predict(s1_batch)\n","    ###### Q-table에서 업데이트는 ? #############################################\n","    ## Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*np.max(Q[s1,:]) - Q[s0, a0]) #\n","    #############################################################################\n","\n","    target_Q_batch[np.arange(batch_size),a0] = r1 + gamma*np.max(Q_s1, axis=1)*(1-d)\n","\n","    return s0_batch, target_Q_batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dilUhdpunC2Z","colab_type":"text"},"source":["# Memory로 부터 학습 하는 Q-Network!"]},{"cell_type":"code","metadata":{"id":"d4AD7VQYEMXA","colab_type":"code","colab":{}},"source":["env.close()\n","\n","alpha = 0.1\n","gamma = 0.9\n","n_episod = 200\n","epsilon = 0.1\n","sample_size = batch_size = 256\n","cum_rewards = []\n","\n","for i in range(n_episod) :\n","    print(\"episode {} --진행 중\".format(i+1))\n","    env = wrap_env(gym.make(\"LunarLander-v2\"))\n","    s0 = env.reset()\n","    s0 = s0.reshape([1, -1]) # 2차원 어레이로 바꿔주기\n","    done = False\n","\n","    cum_r = 0\n","\n","    while True :\n","        Q_s0 = Q_network.predict(s0) #s0에서의 action들의 Q_value\n","        # 행동 선택하기 e-greedy 방법\n","        if np.random.uniform() < epsilon :\n","            a0 = env.action_space.sample()\n","        else : \n","            a0 = np.argmax(Q_s0)\n","\n","        # 환경과 상호작용!\n","        s1, r1, done, _ = env.step(a0)\n","        s1 = s1.reshape([1,-1]) # 2차원 어레이로 바꿔주기\n","\n","        # 메모리에 저장!\n","        memory.append((s0, a0, r1, s1, done))\n","\n","        # 학습을 위해 Experience Replay!\n","\n","        s0_batch, target_Q_batch = create_batch(Q_network, memory, gamma, batch_size=batch_size)\n","        Q_network.fit(s0_batch, target_Q_batch, epochs=1, verbose=0, batch_size=batch_size)\n","        cum_r = cum_r + r1\n","        if done == True : # 종료 되었다면\n","            cum_rewards.append(cum_r)\n","            env.close() # 환경닫고\n","            break # 멈추자.\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","\n","    if (i+1) % 2 == 0 :\n","            print('===========  에피소드 : {}  ============'.format(i+1))\n","            print('최종누적보상 :',cum_r)\n","            print(a0, Q_s0)\n","            plt.plot(cum_rewards)\n","            plt.show()\n","            show_video()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_w5X0LC_v57","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}