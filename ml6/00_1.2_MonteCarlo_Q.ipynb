{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_1.2_MonteCarlo_Q.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNaQsV8IKF+78QFi64XV9OT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3UzaouW5VdZu"},"source":["# Monte Carlo Method : Action - Value estimation\n","\n","**공부할 내용**\n","1. state, action, reward 가 무엇인가요?\n","2. Return이 무엇인가요?\n","3. State-Value Function 이란?\n","4. Action-Value Function 이란?\n","5. Policy 란?\n","\n","**사용할 환경 : 자주 만날 Frozen Lake**\n","\n","![좋은거](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSKrspq5EvLOC2Rv8dfsw1OVqD1WjN5YsmgAHVCVn3x8zLhM5Y8&usqp=CAU)\n"]},{"cell_type":"markdown","metadata":{"id":"hJU-vI1RdB_L"},"source":["# 필요 라이브러리 불러오기."]},{"cell_type":"code","metadata":{"id":"Cby2BD-qdKCS"},"source":["!pip install gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXHT6L_cdSPJ"},"source":["import numpy as np\n","import gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9wZyfzXdTRr"},"source":["gym.envs.registration.register(\n","    id=\"FrozenLake-v3\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name': '4x4', 'is_slippery': False}\n",")\n","\n","# gym.envs.registration.register(\n","#     id=\"FrozenLake-v8\", entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","#     kwargs={'map_name': '8x8', 'is_slippery': True}\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOoQHPoRdhjY"},"source":["env = gym.make('FrozenLake-v3')\n","# env = gym.make('FrozenLake-v8')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7k5VAB9Rdi6g"},"source":["# 일단, 랜덤하게 진행한 한 번의 Episode를 기록해볼까?!\n","\n","* Action 도 기록한다!!"]},{"cell_type":"code","metadata":{"id":"jA8Z6uIHdsQy"},"source":["states = []\n","actions = []\n","rewards = []\n","\n","s0 = env.reset()\n","env.render()\n","while True :\n","    a0 = env.action_space.sample()\n","    s1, r1, done, _ = env.step(a0) # 움직여!\n","\n","    states.append(s0)\n","    actions.append(a0)\n","    rewards.append(r1)\n","    env.render()\n","\n","    if done == True :\n","        env.close() # 환경닫기\n","        break\n","\n","    s0 = s1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2A-IklwPfFWn"},"source":["states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-Zbb4a9fNh4"},"source":["rewards"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LC8E04adfUf_"},"source":["# Return 만들기 연습!"]},{"cell_type":"code","metadata":{"id":"XXGhszcvfXBv"},"source":["Returns = [  ]\n","rewards_sim = [1,2,7,-1,2,-4,5,2,2,-100]\n","gamma = 0.9\n","\n","Gt = 0\n","##### 1.0 참고하여 제작!\n","\n","print(Returns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j6CgmOI-gbMD"},"source":["# State,Action 별로 가치를 관찰해보자."]},{"cell_type":"code","metadata":{"id":"G0YTEkiVgtdW"},"source":["sim_returns = Returns[:len(states)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eht525gqgeHV"},"source":["len(states),len(actions), len(sim_returns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkjRaCpmgnMM"},"source":["print(states)\n","print(actions)\n","print(sim_returns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzsGVEd1hFms"},"source":["# Action-Value (Q)를 일단 제작해보자.\n","\n","V[3][2] 이라고 한다면, 3번 state에서 2번 action을 했을 때의 가치가 나와야 한다!"]},{"cell_type":"code","metadata":{"id":"JUbmfShKhoXz"},"source":["print(env.observation_space)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVp0AgCZhekR"},"source":["Q = np.zeros(shape=(16,4))\n","Q"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDCXT39IjvYp"},"source":["# Incrementally 계산!"]},{"cell_type":"code","metadata":{"id":"CM7SHJzljySj"},"source":["sa_n = np.zeros(shape=(16,4))\n","Q = np.zeros(shape=(16,4))\n","#states, actions, sim_returns 이용하는 것은 맞다.\n","for t, state in enumerate(states):\n","    at = actions[t]\n","    st = state\n","    sa_n[st][at] = sa_n[st][at] + 1\n","    n_sa = sa_n[st][at]\n","    Q[st][at] = Q[st][at] + (1/n_sa)*(sim_returns[t] - Q[st][at])\n","\n","Q"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAS1RV3JnqsN"},"source":["## Total 연습!\n","\n","1. 총 10000번의 episode를 시행한다!\n","    1. 하나의 episode가 진행 중일 땐,\n","        * states = [] # state 기록!\n","        * actions = [] # action 기록\n","        * rewards = [] # reward 기록!\n","    2. 하나의 episode가 종료되면\n","        * returns = [] # $G_t$ 제작, 기록!\n","        * (사실 이 단계에서 $V(s)$를 업데이트 해야 좋음 - episode마다 업데이트가 되니까!)\n","2. episode가 끝나면\n","    * sa_n : state-action별 방문횟수 업데이트 하는 넘파이 어레이 제작\n","    * Q : state-action별 가치값이 저장될 0으로 가득찬 2차원 array제작\n","    * states, sa_n, returns 이용하여 V업데이트!\n","\n","$$Q(S_t, A_t) = Q(S_t, A_t) + {1\\over{N(S_t,A_t)}}(G_t - Q(S_t, A_t))$$"]},{"cell_type":"code","metadata":{"id":"mlQJSYHwjw7_"},"source":["####### Your Code Here ##########\n","\n","states = []\n","actions = []\n","returns = []\n","\n","gamma = 0.9\n","\n","for i in range(10000) :\n","    # 환경을 초기화 하며 초기 state 를 s0에 선언하시오.\n","    s0 = env.reset() \n","    rewards = []\n","    temp_returns = []\n","    while True :\n","        # 랜덤한 액션을 하나 선택하여 a0에 선언하시오.\n","\n","\n","        # a0를 이용하여 환경과 상호작용 하면서, s1, r1, done, _ 를 선언하시오.\n","\n","\n","        # states에 s0을 담고 rewards에 r1을 담으시오.\n","        # a0 도 actions에 담으시오.\n"," \n","\n","\n","        if done == True :\n","            env.close() # 환경닫기\n","            break\n","        # 다음 단계의 s0에 s1의 값을 옮겨 선언하시오.\n","        s0 = s1\n","\n","    ### 이 episode에 대해서 Return을 계산하여 temp_returns에 담고\n","    ### returns = returns + temp_returns\n","    ### temp_returns.append()아님 주의! insert!\n","    Gt = 0\n","    for r in rewards[::-1] : \n","        # Gt 계산!\n","        temp_returns.insert(0, Gt)\n","\n","    returns = returns + temp_returns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-je478DytNnN"},"source":["len(states), len(returns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"na6ipqIBtRay"},"source":["sa_n = np.zeros(shape=(16,4))\n","Q = np.zeros(shape=(16,4))\n","#state, returns 이용\n","for t, state in enumerate(states):\n","    at =          # t 시점의 액션을 선언\n","    st =          # t 시점의 state를 선언해둠\n","    sa_n[st][at] =      # 등장횟수를 업데이트\n","    n_sa =         # t시점까지, (s,a)가 등장한 쌍을 여기에 선언해옴. (윗 줄 이용!)\n","    Q[st][at] =   # 업데이트!\n","\n","Q"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63imBPbhtVOE"},"source":["# 자! Action-Value 바탕으로 움직인다면?\n","\n","바꿔 말해, Policy를 제작한다면?\n","\n","$$\\pi'(s) = argmax_{a}(Q(s,a) )$$"]},{"cell_type":"code","metadata":{"id":"P5DHy9401_81"},"source":["print(Q[4])\n","print(Q[4].argmax())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSj2XTIK2IV8"},"source":["s0 = env.reset()\n","env.render()\n","for i in range(1000):\n","    a0 = Q[s0].argmax()\n","    s1, r1, done, _ = env.step(a0)\n","    env.render()\n","    if done == True :\n","        env.close()\n","        break\n","    s0 = s1"],"execution_count":null,"outputs":[]}]}