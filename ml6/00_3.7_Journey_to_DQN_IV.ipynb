{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_3_Journey_to_DQN_IV.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8tqiq4PMi1VCQka67sahw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZCY7LwMxFN9V","colab_type":"text"},"source":["# Deep Q Network로의 여행.\n","# Part IV. +CNN\n","\n","![좋은 그림](https://d3i71xaburhd42.cloudfront.net/e6a1640c03c50a55ef3e00a0592dbb0851fe33bb/3-Figure1-1.png)\n","\n","[읽어보면 좋은 것 0](https://www.katnoria.com/static/e59c56013a5d82a0ae94d9413076dfc0/1e043/dqn_algo.png)<br>\n","[읽어보면 좋은 것 1](https://arxiv.org/pdf/1312.5602.pdf)<br>\n","[읽어보면 좋은 것 2](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xklO0e3y-5nl","colab_type":"text"},"source":["# 라이브러리 설치 / 불러오기"]},{"cell_type":"code","metadata":{"id":"NJXIDTmawqn6","colab_type":"code","colab":{}},"source":["%%time\n","## 약 25초 ~30초 소요\n","!pip install pyvirtualdisplay \n","!apt-get install -y xvfb python-opengl ffmpeg\n","!pip install gym\n","!pip install box2d-py\n","#!pip install pyglet==1.3.2\n","!pip install pyglet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmOw7kOJ-0ht","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4JtH_2M-3YY","colab_type":"code","colab":{}},"source":["display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGlgV8N3-4Qj","colab_type":"text"},"source":["비디오 녹화용 함수"]},{"cell_type":"code","metadata":{"id":"XuIFZVbo--K_","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[-1]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sArxiosg--cU","colab_type":"text"},"source":["# SpaceInvaders-v0"]},{"cell_type":"code","metadata":{"id":"lYyFQyA9_Fys","colab_type":"code","colab":{}},"source":["env = wrap_env(gym.make(\"SpaceInvaders-v0\"))\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)\n","\n","rewards_cums = []\n","rewards_cum = 0\n","state = env.reset()\n","for t in range(1000):\n","    action = env.action_space.sample() # your agent here (this takes random actions)\n","    env.render()\n","    observation, reward, done, info = env.step(action)\n","    rewards_cum = rewards_cum + reward\n","    rewards_cums.append(rewards_cum)\n","    if done: \n","      break;\n","            \n","print('steps: ', t)\n","env.close()\n","show_video()\n","plt.plot(rewards_cums)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zixhWn5quNmQ","colab_type":"code","colab":{}},"source":["plt.imshow(env.reset()[25:-14,4:-4,:])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5UOv3qZeqrNB","colab_type":"text"},"source":["# 전처리가 필요하다!\n","\n","1. 컬러 --> 그레이스케일(흑백) : 조금이라도 덜 복잡하도록.\n","2. 필요 없는 하단(에이전트 아래)을 제거\n","3. 픽셀 값 min-max scaling\n","4. [110,84] 사이즈로 resize : 조금이라도 사이즈가 작도록."]},{"cell_type":"code","metadata":{"id":"xdyQoWiru45n","colab_type":"code","colab":{}},"source":["from skimage import transform\n","from skimage.color import rgb2gray"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ye5hfR66qrHJ","colab_type":"code","colab":{}},"source":["def preprocess_frame(frame):\n","    gray = rgb2gray(frame)\n","    cropped_frame = gray[25:-14,4:-4]\n","    scaled_frame = cropped_frame/255\n","    resized_frame = transform.resize(scaled_frame, [110, 84])\n","\n","    return resized_frame"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rnfZnbAav5la","colab_type":"text"},"source":["# Stack Frame\n","\n","1. 연속된 4개의 이미지를 묶는다. '모션'을 인식시킬 수 있지 않을까?"]},{"cell_type":"code","metadata":{"id":"UnnAzl0mxbJj","colab_type":"code","colab":{}},"source":["from collections import deque"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0QUecHRwm8Q","colab_type":"code","colab":{}},"source":["stack_size = 4\n","\n","stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n","\n","def stack_frames(stacked_frames, state, is_new_episode):\n","    # Preprocess frame\n","    frame = preprocess_frame(state)\n","    \n","    if is_new_episode:\n","        # 새로 시작하는 상황이라면, stacked_frames를 비우고 새로 만든다.\n","        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n","        \n","        # 그리고 똑같은 시작 state(frame)을 4개 스택!\n","        stacked_frames.append(frame)\n","        stacked_frames.append(frame)\n","        stacked_frames.append(frame)\n","        stacked_frames.append(frame)\n","        stacked_state = np.stack(stacked_frames, axis=2).reshape([-1,110,84,4])\n","        \n","    else:\n","        # 자동으로 오래된 프레임은 날린다.\n","        stacked_frames.append(frame)\n","        stacked_state = np.stack(stacked_frames, axis=2).reshape([-1,110,84,4])\n","    \n","    return stacked_state, stacked_frames"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRgflNf4_Lgg","colab_type":"text"},"source":["# Convolutional Neural Network for Q-function\n","\n","**Q-function기능을 할 뉴럴넷을 구성할 것이다.**\n","1. input은 state다. (노드 수는?)\n","2. output은 그 state에서 취할 수 있는 action에 대한 Q값이다. (노드 수는?)\n","\n","[좋은그림](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9911AF455B9A0A9706)<br>\n","[좋은그림1](https://theaisummer.com/assets/img/posts/Taking_Deep_Q_Networks_a_step_further/DDQN.jpg)\n","\n","![좋은그림1](https://i.ytimg.com/vi/2-zGCx4iv_k/hqdefault.jpg)\n"]},{"cell_type":"code","metadata":{"id":"3YuvdNAp_87B","colab_type":"code","colab":{}},"source":["print(\"state 수는? : \", env.observation_space.shape)\n","print(\"action 수는? : \", env.action_space.n)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekpE4onvBihd","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Input, Dense, Add, Conv2D, Flatten, Lambda\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras.utils import plot_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-hcSe2lVvYz","colab_type":"code","colab":{}},"source":["### 이것은 일단 그냥 사용해보자!\n","### huber 로스!\n","from tensorflow.keras.losses import Huber\n","def mean_huber_loss(y_true, y_pred, clip_delta=1.0):\n","    return tf.keras.backend.mean(Huber(clip_delta)(y_true, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3v8zZtzSBTcV","colab_type":"code","colab":{}},"source":["keras.backend.clear_session()\n","######################\n","### Your Code Here ###\n","######################\n","\n","state_layer = Input(shape=(110,84,4), name='State_frames')\n","hc = Conv2D(32, kernel_size=(8,8), strides=(4,4),\n","            padding='same', activation='elu', name='Conv1')(state_layer)\n","hc = Conv2D(64, kernel_size=(4,4), strides=(2,2),\n","            padding='valid', activation='elu', name='Conv2')(hc)\n","hc = Conv2D(64, kernel_size=(3,3), strides=(2,2),\n","            padding='valid', activation='elu', name='Conv3')(hc)\n","hidden = Flatten()(hc)\n","\n","hv = Dense(512, activation='relu')(hidden)\n","hv = Dense(256, activation='relu')(hv)\n","v_layer = Dense(1, name='V')(hv)\n","\n","ha = Dense(512, activation='relu')(hidden)\n","ha = Dense(512, activation='relu')(ha)\n","a_layer = Dense(env.action_space.n)(ha)\n","a_layer = Lambda(lambda a: a[:, :] - tf.keras.backend.mean(a[:,:],axis=1, keepdims=True ),name='Advantage')(a_layer)\n","\n","q_layer = Add(name=\"Q\")([v_layer, a_layer]) #Broadcast 해준다\n","\n","Q_network = Model(state_layer, q_layer)\n","\n","Q_network.compile(loss = mean_huber_loss,\n","              optimizer = Adam())\n","\n","# Q_network.summary()\n","\n","plot_model(Q_network, show_shapes=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KOT30gwQccQ","colab_type":"text"},"source":["### 위 코드를 보고 target_Q_network를 구성하시오.\n","\n","* 똑같으면 된다.\n","* 단, 모델 이름은 target_Q_network"]},{"cell_type":"code","metadata":{"id":"F5MgtWdIQf8c","colab_type":"code","colab":{}},"source":["# keras.backend.clear_session()\n","######################\n","### Your Code Here ###\n","######################\n","\n","state_layer = Input(shape=(110,84,4), name='State_frames')\n","hc = Conv2D(32, kernel_size=(8,8), strides=(4,4),\n","            padding='same', activation='elu', name='Conv1')(state_layer)\n","hc = Conv2D(64, kernel_size=(4,4), strides=(2,2),\n","            padding='valid', activation='elu', name='Conv2')(hc)\n","hc = Conv2D(64, kernel_size=(3,3), strides=(2,2),\n","            padding='valid', activation='elu', name='Conv3')(hc)\n","hidden = Flatten()(hc)\n","\n","hv = Dense(512, activation='relu')(hidden)\n","hv = Dense(256, activation='relu')(hv)\n","v_layer = Dense(1, name='V')(hv)\n","\n","ha = Dense(512, activation='relu')(hidden)\n","ha = Dense(512, activation='relu')(ha)\n","a_layer = Dense(env.action_space.n)(ha)\n","a_layer = Lambda(lambda a: a[:, :] - tf.keras.backend.mean(a[:,:],axis=1, keepdims=True ),name='Advantage')(a_layer)\n","\n","q_layer = Add(name=\"Q\")([v_layer, a_layer]) #Broadcast 해준다\n","\n","target_Q_network = Model(state_layer, q_layer)\n","\n","target_Q_network.compile(loss = mean_huber_loss,\n","              optimizer = Adam())\n","\n","# Q_network.summary()\n","\n","plot_model(target_Q_network, show_shapes=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGT1J7CHQf-z","colab_type":"code","colab":{}},"source":["## Target Q 네트워크의 가중치는 같아야 한다.\n","target_Q_network.set_weights(Q_network.get_weights())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7f5syqDCFqD","colab_type":"text"},"source":["# Memory 구현 & Memory에 어느정도 Experience를 담아두기!\n","\n","1. (s0, a0, r1, s1, done)을 담아두면 된다. 튜플!\n","2. deque를 이용하여 최근 n개의 Experience만 담아둘수 있도록 한다.\n","3. Experience 를 replay하며 배울 때는..\n","    * sample_size = bach_size = 128 개씩 경험을 랜덤추출하여 .fit()할 것이다!\n","    * 따라서 experience(s0, a0, r1, s1, done)은 충분히 미리 담아두자.\n","    * 랜덤액션으로 담아두어도 좋다."]},{"cell_type":"code","metadata":{"id":"cktYnNxYlarr","colab_type":"code","colab":{}},"source":["from collections import deque\n","memory = deque(maxlen = 20000)  # 리스트 처럼 사용이 가능하다."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKFhfBBanC-n","colab_type":"code","colab":{}},"source":["alpha = 0.1\n","gamma = 0.95\n","n_episod = 100\n","epsilon = 0.1\n","pre_play = 10 # 실제 담기는건 10개가 아님.\n","\n","for i in range(pre_play) :\n","    print(i, \" 번째 에피소드,\")\n","    s0 = env.reset()\n","    s0, stacked_frames = stack_frames(stacked_frames, s0, True)\n","    done = False\n","\n","    while True :\n","\n","        a0 = env.action_space.sample() # 랜덤 액션!\n","        # 환경과 상호작용!\n","        s1, r1, done, _ = env.step(a0)\n","        s1, stacked_frames = stack_frames(stacked_frames, s1, False)\n","\n","        if done == False :\n","            memory.append( (s0, a0, r1, s1, done) )\n","            s0 = s1\n","        else :\n","            s1 = np.zeros(s0.shape)  # 끝나면 s1가 0이 됨!\n","            memory.append( (s0, a0, r1, s1, done) )\n","            env.close()\n","            break\n","    print(\"저장된 experience : {}\".format(len(memory)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpr7DtQAwe2r","colab_type":"text"},"source":["# Memory로 부터 배치 사이즈 만큼 데이터를 떼어 오는 함수 제작\n","\n","* .fit(s0, target_Q) 임을 고려하자\n","* 심지어 이제 진짜 target_Q_network 를 이용하여 만들어야 하는것이 맞다.\n"]},{"cell_type":"code","metadata":{"id":"qH-Pkyzl1AEr","colab_type":"code","colab":{}},"source":["import random\n","def create_batch(target_model, memory, gamma, batch_size = 128):\n","    sample = np.array(random.sample(memory, batch_size))\n","\n","    s0 = sample[:, 0]\n","    a0 = sample[:, 1].astype(np.int8)\n","    r1 = sample[:, 2]\n","    s1 = sample[:, 3]\n","    d = sample[:, 4]\n","\n","    s0_batch = np.vstack(s0)\n","    s1_batch = np.vstack(s1)\n","\n","    target_Q_batch = target_model.predict(s0_batch)\n","    Q_s1 = target_model.predict(s1_batch)\n","    ###### Q-table에서 업데이트는 ? #############################################\n","    ## Q[s0, a0] = Q[s0, a0] + alpha * (r1 + gamma*np.max(Q[s1,:]) - Q[s0, a0]) #\n","    #############################################################################\n","\n","    target_Q_batch[np.arange(batch_size),a0] = r1 + gamma*np.max(Q_s1, axis=1)*(1-d)\n","\n","    return s0_batch, target_Q_batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dilUhdpunC2Z","colab_type":"text"},"source":["# Memory로 부터 학습 하는 Q-Network!"]},{"cell_type":"code","metadata":{"id":"d4AD7VQYEMXA","colab_type":"code","colab":{}},"source":["env.close()\n","alpha = 0.1\n","gamma = 0.9\n","n_episod = 1000\n","\n","epsilon = 1\n","\n","sample_size = batch_size = 64\n","cum_rewards = []\n","tq_update = 10 ## target_Q는 언제 업데이트 할래?\n","\n","for i in range(n_episod) :\n","    print(\"episode {} --진행 중\".format(i+1))\n","    env = wrap_env(gym.make(\"SpaceInvaders-v0\"))\n","    s0 = env.reset()\n","    s0, stacked_frames = stack_frames(stacked_frames, s0, True)\n","    done = False\n","\n","    cum_r = 0\n","    time_step = 0\n","    while True :\n","        Q_s0 = Q_network.predict(s0) #s0에서의 action들의 Q_value\n","        epsilon = max(0.1, epsilon*0.9995)\n","        # 행동 선택하기 e-greedy 방법\n","        if np.random.uniform() < epsilon :\n","            a0 = env.action_space.sample()\n","        else : \n","            a0 = np.argmax(Q_s0)\n","\n","        # 환경과 상호작용!\n","        s1, r1, done, _ = env.step(a0)\n","        s1, stacked_frames = stack_frames(stacked_frames, s1, False)\n","\n","        # 메모리에 저장!\n","        memory.append((s0, a0, r1, s1, done))\n","        # 학습을 위해 Experience Replay!\n","        # create_batch 조심.\n","        s0_batch, target_Q_batch = create_batch(target_Q_network, memory, gamma, batch_size=batch_size) \n","        Q_network.fit(s0_batch, target_Q_batch, epochs=0, verbose=1, batch_size=batch_size)\n","\n","        cum_r = cum_r + r1\n","\n","        time_step = time_step + 1\n","        if time_step % tq_update == 0 :\n","            ## Target Q 네트워크의 가중치 업데이트\n","            target_Q_network.set_weights(Q_network.get_weights())\n","\n","        if done == True : # 종료 되었다면\n","            cum_rewards.append(cum_r)\n","            env.close() # 환경닫고\n","            ## Target Q 네트워크의 가중치 업데이트\n","            target_Q_network.set_weights(Q_network.get_weights())\n","            break # 멈추자.\n","\n","        s0 = s1 # 다음 루프에선 이것이 직전 state\n","\n","    if (i+1) % 2 == 0 :\n","        print('===========  에피소드 : {}  ============'.format(i+1))\n","        print('최종누적보상 :',cum_r)\n","        print(a0, Q_s0)\n","        plt.plot(cum_rewards)\n","        plt.show()\n","        show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_w5X0LC_v57","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}